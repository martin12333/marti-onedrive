{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563f46a8-5708-4093-8d7a-300f9aa2b7ff",
   "metadata": {},
   "source": [
    "# SVD weight directions are highly interpretable\n",
    "https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ca6fb-b7c6-4c47-8006-76a8f6f4c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini nb  #mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e12dc-84be-4079-a706-eca443065c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0195d2-71a7-4925-9b90-803aba66f37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d63a523-7b43-48fb-a28b-3c0916f3a128",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIThWRFlvryT",
    "outputId": "46302aba-9964-4578-ef47-6db9aa94fa51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get everything set up\n",
    "# more rapidly install node\n",
    "\n",
    "#!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "# install other dependencies\n",
    "\n",
    "#!pip install transformers\n",
    "\n",
    "#%pip install  --dry-run  datasets\n",
    "\n",
    "# install repo with the data\n",
    "#!git clone https://github.com/BerenMillidge/svd_directions\n",
    "\n",
    "\n",
    "#aaa\n",
    "#%pip install   --dry-run  tabulate\n",
    "#%pip install     tabulate\n",
    "#!bash setup.sh\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f245e06b-8a71-49a7-968f-d393db244fac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIThWRFlvryT",
    "outputId": "46302aba-9964-4578-ef47-6db9aa94fa51"
   },
   "outputs": [],
   "source": [
    "\n",
    "#%cd svd_directions\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm, trange\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# utils\n",
    "import json\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from copy import deepcopy\n",
    "from torch.nn import functional as F\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm, trange\n",
    "import functools\n",
    "import math\n",
    "\n",
    "# this resets up the site so you don't have to restart the runtime to use pysvelte\n",
    "#import site\n",
    "#site.main()\n",
    "#import pysvelte\n",
    "\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "cmap = sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fda13-775b-4636-9e54-d286c5a468b7",
   "metadata": {
    "id": "Atx1oVp8xgHy",
    "tags": []
   },
   "source": [
    "# token utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a94538dc-f239-4121-a103-46479ffab215",
   "metadata": {
    "id": "O-Fci9FTwq9b"
   },
   "outputs": [],
   "source": [
    "def keep_k(x, k=100, absolute=True, dim=-1):\n",
    "    shape = x.shape\n",
    "    x_ = x\n",
    "    if absolute:\n",
    "        x_ = abs(x)\n",
    "    values, indices = torch.topk(x_, k=k, dim=dim)\n",
    "    res = torch.zeros_like(x)\n",
    "    res.scatter_(dim, indices, x.gather(dim, indices))\n",
    "    return res\n",
    "\n",
    "def get_max_token_length(tokens):\n",
    "  maxlen = 0\n",
    "  for t in tokens:\n",
    "    l = len(t)\n",
    "    if l > maxlen:\n",
    "      maxlen = l\n",
    "  return maxlen\n",
    "\n",
    "def pad_with_space(t, maxlen):\n",
    "  spaces_to_add = maxlen - len(t)\n",
    "  for i in range(spaces_to_add):\n",
    "    t += \" \"\n",
    "  return t\n",
    "\n",
    "def convert_to_tokens(indices, tokenizer, extended, extra_values_pos, strip=True, pad_to_maxlen=False):\n",
    "    if extended:\n",
    "        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else \n",
    "               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\") \n",
    "               for idx in indices]\n",
    "    else:\n",
    "        res = tokenizer.convert_ids_to_tokens(indices)\n",
    "    if strip:\n",
    "        res = list(map(lambda x: x[1:] if x[0] == 'Ġ' else \"#\" + x, res))\n",
    "    if pad_to_maxlen:\n",
    "      maxlen = get_max_token_length(res)\n",
    "      res = list(map(lambda t: pad_with_space(t, maxlen), res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_tokens(v_tok, k=100, tokenizer=None, only_english=False, only_ascii=True, with_values=False, \n",
    "               exclude_brackets=False, extended=True, extra_values=None, pad_to_maxlen=False):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    v_tok = deepcopy(v_tok)\n",
    "    ignored_indices = []\n",
    "    if only_ascii:\n",
    "        ignored_indices = [key for val, key in tokenizer.vocab.items() if not val.strip('Ġ').isascii()]\n",
    "    if only_english: \n",
    "        ignored_indices =[key for val, key in tokenizer.vocab.items() if not (val.strip('Ġ').isascii() and val.strip('Ġ[]').isalnum())]\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "    v_tok[ignored_indices] = -np.inf\n",
    "    extra_values_pos = len(v_tok)\n",
    "    if extra_values is not None:\n",
    "        v_tok = torch.cat([v_tok, extra_values])\n",
    "    values, indices = torch.topk(v_tok, k=k)\n",
    "    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos,pad_to_maxlen = pad_to_maxlen)\n",
    "    if with_values:\n",
    "        res = list(zip(res, values.cpu().numpy()))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_matrix_tokens(mat, k=100, tokenizer=None, rel_thresh=None, thresh=None, \n",
    "                      sample_entries=10000, alphabetical=True, only_english=False,\n",
    "                      exclude_brackets=False, with_values=True, extended=True):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    mat = deepcopy(mat)\n",
    "    ignored_indices = []\n",
    "    if only_english:\n",
    "        ignored_indices = [key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.strip('[]').isalnum())]\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "    mat[ignored_indices, :] = -np.inf\n",
    "    mat[:, ignored_indices] = -np.inf\n",
    "    cond = torch.ones_like(mat).bool()\n",
    "    if rel_thresh:\n",
    "        cond &= (mat > torch.max(mat) * rel_thresh)\n",
    "    if thresh:\n",
    "        cond &= (mat > thresh)\n",
    "    entries = torch.nonzero(cond)\n",
    "    if sample_entries:\n",
    "        entries = entries[np.random.randint(len(torch.nonzero(cond)), size=sample_entries)]\n",
    "    res_indices = sorted(entries, \n",
    "                         key=lambda x: x[0] if alphabetical else -mat[x[0], x[1]])\n",
    "    res = [*map(partial(convert_to_tokens, extended=extended, tokenizer=tokenizer), res_indices)]\n",
    "            \n",
    "    if with_values:\n",
    "        res_ = []\n",
    "        for (x1, x2), (i1, i2) in zip(res, res_indices):\n",
    "            res_.append((x1, x2, mat[i1][i2].item()))\n",
    "        res = res_    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8d9e7-2212-4f40-a99b-784e81f4235e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1cfab33-9078-4138-8ae4-7b655d18729e",
   "metadata": {
    "id": "o3LCx8JvxiOi",
    "tags": []
   },
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec7b392-eaef-41e2-ae8d-88326ec67f0a",
   "metadata": {
    "id": "hpD59bpfxndh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def rgetattr(obj, attr, *args):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "def get_model_tokenizer_embedding(model_name):\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  if device == 'cpu':\n",
    "    print(\"WARNING: you should probably restart on a GPU runtime\")\n",
    "\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  emb = model.get_output_embeddings().weight.data.T.detach()\n",
    "  return model, tokenizer, emb, device\n",
    "\n",
    "\n",
    "def get_model_info(model):\n",
    "  num_layers = model.config.n_layer\n",
    "  num_heads = model.config.n_head\n",
    "  hidden_dim = model.config.n_embd\n",
    "  head_size = hidden_dim // num_heads\n",
    "  return num_layers, num_heads, hidden_dim, head_size\n",
    "\n",
    "def get_mlp_weights(model,num_layers, hidden_dim):\n",
    "  Ks = []\n",
    "  Vs = []\n",
    "  for j in range(num_layers):\n",
    "    K = model.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T.detach()\n",
    "    # fuse the layernorm\n",
    "    ln_2_weight = model.get_parameter(f\"transformer.h.{j}.ln_2.weight\").detach()\n",
    "    K = torch.einsum(\"oi,i -> oi\", K, ln_2_weight)\n",
    "    \n",
    "    V = model.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n",
    "    Ks.append(K)\n",
    "    Vs.append(V)\n",
    "  \n",
    "  Ks =  torch.cat(Ks)\n",
    "  Vs = torch.cat(Vs)\n",
    "  K_heads = Ks.reshape(num_layers, -1, hidden_dim)\n",
    "  V_heads = Vs.reshape(num_layers, -1, hidden_dim)\n",
    "  return K_heads, V_heads\n",
    "\n",
    "def get_attention_heads(model, num_layers, hidden_dim, num_heads, head_size):\n",
    "  qkvs = []\n",
    "  for j in range(num_layers):\n",
    "    qkv = model.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\").detach().T\n",
    "    ln_weight_1 = model.get_parameter(f\"transformer.h.{j}.ln_1.weight\").detach()\n",
    "    \n",
    "    qkv = qkv - torch.mean(qkv, dim=0) \n",
    "    qkv = torch.einsum(\"oi,i -> oi\", qkv, ln_weight_1)\n",
    "    qkvs.append(qkv.T)\n",
    "\n",
    "  W_Q, W_K, W_V = torch.cat(qkvs).chunk(3, dim=-1)\n",
    "  W_O = torch.cat([model.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\") for j in range(num_layers)]).detach()\n",
    "  W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "  W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)\n",
    "  W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "  W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "  return W_Q_heads, W_K_heads, W_V_heads, W_O_heads\n",
    "\n",
    "def top_singular_vectors(mat, emb, all_tokens, k = 20, N_singular_vectors = 10, with_negative = False,use_visualization=True, filter=\"topk\"):\n",
    "  U,S,V = torch.linalg.svd(mat)\n",
    "  Vs = []\n",
    "  for i in range(N_singular_vectors):\n",
    "      acts = V[i,:].float() @ emb\n",
    "      Vs.append(acts)\n",
    "  if use_visualization:\n",
    "    Vs = torch.stack(Vs, dim=1).unsqueeze(1) # n_tokens, n_layers (1), n_directions\n",
    "    pysvelte.TopKTable(tokens=all_tokens, activations=Vs, obj_type=\"SVD direction\", k=k, filter=filter).show()\n",
    "  else:\n",
    "    Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n",
    "    print(tabulate([*zip(*Vs)]))\n",
    "  if with_negative:\n",
    "    Vs = []\n",
    "    for i in range(N_singular_vectors):\n",
    "      acts = -V[i,:].float() @ emb\n",
    "      Vs.append(acts)\n",
    "    if use_visualization:\n",
    "      Vs = torch.stack(Vs, dim=1).unsqueeze(1) # n_tokens, n_layers (1), n_directions\n",
    "      pysvelte.TopKTable(tokens=all_tokens, activations=Vs, obj_type=\"SVD direction\", k=k, filter=filter).show()\n",
    "    else:\n",
    "      Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n",
    "      print(tabulate([*zip(*Vs)]))\n",
    "\n",
    "def plot_MLP_singular_vectors(K,layer_idx, max_rank=None):\n",
    "  W_matrix = K[layer_idx, :,:]\n",
    "  U,S,V = torch.linalg.svd(W_matrix,full_matrices=False)\n",
    "  if not max_rank:\n",
    "    max_rank = len(S)\n",
    "  if max_rank > len(S):\n",
    "    max_rank = len(S) -1\n",
    "  plt.plot(S[0:max_rank].detach().cpu().numpy())\n",
    "  plt.yscale('log')\n",
    "  plt.ylabel(\"Singular value\")\n",
    "  plt.xlabel(\"Rank\")\n",
    "  plt.title(\"Distribution of the singular vectors\")\n",
    "  plt.show()\n",
    "\n",
    "def cosine_sim(x,y):\n",
    "    return torch.dot(x,y) / (torch.norm(x) * torch.norm(y))\n",
    "\n",
    "\n",
    "def normalize_and_entropy(V, eps=1e-6):\n",
    "    absV = torch.abs(V)\n",
    "    normV = absV / torch.sum(absV)\n",
    "    entropy = torch.sum(normV * torch.log(normV + eps)).item()\n",
    "    return -entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd5ca7-018b-4f12-9931-648828677279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "016a78c8-01ba-4272-a6e9-85766adb8271",
   "metadata": {
    "id": "ekeOLatgdxhi",
    "tags": []
   },
   "source": [
    "# Visualizing the SVD directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2409f-67a7-4ca6-ae4f-39a114adcf51",
   "metadata": {
    "id": "lkHH61Jvd0av"
   },
   "source": [
    "With the basic code and utility functions setup, we can start visualizing the top singular vectors of the various weight matrices in our transformer model. We find that the singular vectors of both the OV circuit and the MLP layers (especially the MLP $W_{\\text{in}}$) matrices appear to give mostly highly interpretable clusters when projected to latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d01a7412-062f-44dc-9f1f-10d5b8e8aa7c",
   "metadata": {
    "id": "jBppCQTf57ne"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: you should probably restart on a GPU runtime\n"
     ]
    }
   ],
   "source": [
    "# Load up the model and get all the key weight matrices.\n",
    "#model, tokenizer, emb, device = get_model_tokenizer_embedding(model_name=\"gpt2\")\n",
    "model, tokenizer, emb, device = get_model_tokenizer_embedding(model_name=\"gpt2-medium\")\n",
    "my_tokenizer = tokenizer\n",
    "num_layers, num_heads, hidden_dim, head_size = get_model_info(model)\n",
    "all_tokens = [tokenizer.decode([i]) for i in range(tokenizer.vocab_size)]\n",
    "\n",
    "K,V = get_mlp_weights(model, num_layers = num_layers, hidden_dim = hidden_dim)\n",
    "W_Q_heads, W_K_heads, W_V_heads, W_O_heads = get_attention_heads(model, num_layers=num_layers, hidden_dim=hidden_dim, num_heads=num_heads, head_size = head_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3fae3-f84c-40ef-ad11-042ea6d80bdd",
   "metadata": {
    "id": "ELdYODbHfBUd"
   },
   "source": [
    "Let's check out some OV circuits. For instance in block 22, head 10, we find these clusters. The way to read these tables is that the columns each represent a singular vector, ordered from that of the highest singular vector down to the lowest. The rows are the top-k token activations when the singular vector dimension is projected to token space, ordered by their value from top (greatest) to bottom (lowest). The colors represent the strength of the embedding.\n",
    "\n",
    "Feel free to check out other layers and heads to get a real feel for what the full distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98905a72-d0a3-4eff-b66f-cfb6e1120209",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "mGEAEwKAa24V",
    "outputId": "10d889a7-5192-4f8a-efa4-b1b802f9363c"
   },
   "outputs": [],
   "source": [
    "def OV_top_singular_vectors(W_V_heads, W_O_heads, emb, layer_idx, head_idx, all_tokens, k=20, N_singular_vectors=10, use_visualization=True, with_negative=False, filter=\"topk\", return_OV=False,     file=file):\n",
    "  W_V_tmp, W_O_tmp = W_V_heads[layer_idx, head_idx, :], W_O_heads[layer_idx, head_idx]\n",
    "  OV = W_V_tmp @ W_O_tmp\n",
    "  U,S,V = torch.linalg.svd(OV)\n",
    "\n",
    "\n",
    "#list_of_lists = [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]\n",
    "  #with open('output.txt', 'w') as file:\n",
    "  if True:      \n",
    "      print('\\n\\n\\n\\n\\n//mm-l-h',layer_idx,head_idx,file=file)\n",
    "      #print('\\n\\n\\n\\n\\n\\\\lh',layer_idx,head_idx,file=file)\n",
    "      #file.write(' '.join(inner_list) + '\\n')\n",
    "\n",
    "      Vs = []\n",
    "    \n",
    "      for i in range(N_singular_vectors):\n",
    "          acts = V[i,:].float() @ emb\n",
    "          Vs.append(acts)\n",
    "\n",
    "\n",
    "      if use_visualization:\n",
    "        Vs = torch.stack(Vs, dim=1).unsqueeze(1) # n_tokens, n_layers (1), n_directions\n",
    "        pysvelte.TopKTable(tokens=all_tokens, activations=Vs, obj_type=\"SVD direction\", k=k, filter=filter).show()\n",
    "      else:\n",
    "        #Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n",
    "        Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n",
    "\n",
    "\n",
    "        #print(tabulate([*zip(*Vs)]))\n",
    "        #print(([*zip(*Vs)]))\n",
    "        #print((Vs))\n",
    "        for inner_list in Vs:\n",
    "            file.write(' '.join(inner_list) + '\\n')\n",
    "        \n",
    "    \n",
    "    \n",
    "      if with_negative:\n",
    "        Vs = []\n",
    "        for i in range(N_singular_vectors):\n",
    "          acts = -V[i,:].float() @ emb\n",
    "          Vs.append(acts)\n",
    "        if use_visualization:\n",
    "          Vs = torch.stack(Vs, dim=1).unsqueeze(1) # n_tokens, n_layers (1), n_directions\n",
    "          pysvelte.TopKTable(tokens=all_tokens, activations=Vs, obj_type=\"SVD direction\", k=k, filter=filter).show()\n",
    "        else:\n",
    "          Vs = [top_tokens(Vs[i].float().cpu(), k = k, pad_to_maxlen=True) for i in range(len(Vs))]\n",
    "          #print(tabulate([*zip(*Vs)]))\n",
    "          #print(([*zip(*Vs)]))\n",
    "          #print(45)\n",
    "          for inner_list in Vs:\n",
    "                file.write(' '.join(inner_list) + '\\n')\n",
    "        \n",
    "  if return_OV:\n",
    "    return OV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#OV_top_singular_vectors(W_V_heads, W_O_heads, emb, layer_idx=22, head_idx=10,k=20, N_singular_vectors=15, all_tokens = all_tokens)\n",
    "#OV_top_singular_vectors(W_V_heads, W_O_heads, emb, layer_idx=9, head_idx=1,k=20, N_singular_vectors=10, all_tokens = all_tokens,    use_visualization=False ,with_negative=True )\n",
    "\n",
    "with open('output.txt', 'w') as file:\n",
    " for layer_idx in   range(num_layers-6, num_layers):  # range(8,12):  #range(11,12):  #    range(8,12):\n",
    "  for head_idx in  range(num_heads): #  range(num_heads): #range(2):  # range(12):\n",
    "    OV_top_singular_vectors(W_V_heads, W_O_heads, emb, layer_idx=layer_idx, head_idx=head_idx,k=30, N_singular_vectors=20, all_tokens = all_tokens,    use_visualization=False ,with_negative=True ,    file=file )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddb906-bf23-4c92-b74e-feb74eb58b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028de826-984f-415b-a171-79302297a8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dd70f-a30b-4945-9438-1361dfa438f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62bd8f-0f16-4eac-9385-4619d9eaf74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e34d29f-c387-41a1-a6b2-1c35e7376a9a",
   "metadata": {},
   "source": [
    ">>> print(tabulate(table, headers, tablefmt=\"html\"))\n",
    "html produces standard HTML markup as an html.escape'd str with a .repr_html method so that Jupyter Lab and Notebook display the HTML and a .str property so that the raw HTML remains accessible. unsafehtml table format can be used if an unescaped HTML is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e233a8d-70ce-48d0-9e3d-4fe44201d1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
