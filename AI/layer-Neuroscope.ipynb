{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54fae4f-77e6-4c13-8471-fc7cfe483082",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://platform.openai.com/playground/p/JDgU4QisbgFa8ZhDIqkw0wP9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69fe8e1a-5fd8-4ef6-9769-39d92cef85d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_32100\\1796901329.py:16: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_32100\\1796901329.py:17: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c9e27-2874-4703-a786-84c269918f7d",
   "metadata": {},
   "source": [
    "\n",
    "I am editing a Python Jupyter notebook called \"Interactive Neuroscope\". It uses Huggingface Transformers.\n",
    "\n",
    "```\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "```\n",
    "\n",
    " (the loaded model is gpt-2-small )\n",
    "\n",
    "The author wrote:\n",
    "\n",
    "> This is an interactive accompaniment to neuroscope.io and to the studying learned language features post in 200 Concrete Open Problems in Mechanistic Interpretability\n",
    "\n",
    "My question is:\n",
    "\n",
    "how can I get a tensor with neuron activations ... e.g. from the layer 9, on a given text ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5ecc3c-4ad5-4292-80f6-560d9d5414fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4e43b-c689-4f4a-b1d0-d8c50e2b6a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626d8016-319a-4fc7-84da-8d7f16ff9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and tokenizer\n",
    "name='gpt2'\n",
    "config = AutoConfig.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForCausalLM.from_pretrained(name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "031af2a1-6441-4d2a-a634-478a1606cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the text you would like to generate the activation tensor for\n",
    "text = 'This is the text I would like to get the activation tensor for.'\n",
    "\n",
    "# Encode the text into tokens\n",
    "input_ids = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6797020b-ffe9-4204-b1d2-0edfa18b2a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpast_key_values\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mposition_ids\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0muse_cache\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCausalLMOutputWithCrossAttentions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "The [`GPT2LMHeadModel`] forward method, overrides the `__call__` special method.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
       "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
       "the latter silently ignores them.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Args:\n",
       "    input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n",
       "        `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n",
       "        `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n",
       "        sequence tokens in the vocabulary.\n",
       "\n",
       "        If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n",
       "        `input_ids`.\n",
       "\n",
       "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
       "        [`PreTrainedTokenizer.__call__`] for details.\n",
       "\n",
       "        [What are input IDs?](../glossary#input-ids)\n",
       "    past_key_values (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`):\n",
       "        Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see\n",
       "        `past_key_values` output below). Can be used to speed up sequential decoding. The `input_ids` which have\n",
       "        their past given to this model should not be passed as `input_ids` as they have already been computed.\n",
       "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
       "\n",
       "        - 1 for tokens that are **not masked**,\n",
       "        - 0 for tokens that are **masked**.\n",
       "\n",
       "        If `past_key_values` is used, `attention_mask` needs to contain the masking strategy that was used for\n",
       "        `past_key_values`. In other words, the `attention_mask` always has to have the length:\n",
       "        `len(past_key_values) + len(input_ids)`\n",
       "\n",
       "        [What are attention masks?](../glossary#attention-mask)\n",
       "    token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n",
       "        Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
       "        1]`:\n",
       "\n",
       "        - 0 corresponds to a *sentence A* token,\n",
       "        - 1 corresponds to a *sentence B* token.\n",
       "\n",
       "        [What are token type IDs?](../glossary#token-type-ids)\n",
       "    position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "        Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
       "        config.max_position_embeddings - 1]`.\n",
       "\n",
       "        [What are position IDs?](../glossary#position-ids)\n",
       "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
       "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
       "\n",
       "        - 1 indicates the head is **not masked**,\n",
       "        - 0 indicates the head is **masked**.\n",
       "\n",
       "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
       "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
       "        is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
       "        model's internal embedding lookup matrix.\n",
       "\n",
       "        If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see\n",
       "        `past_key_values`).\n",
       "    use_cache (`bool`, *optional*):\n",
       "        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
       "        `past_key_values`).\n",
       "    output_attentions (`bool`, *optional*):\n",
       "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
       "        tensors for more detail.\n",
       "    output_hidden_states (`bool`, *optional*):\n",
       "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
       "        more detail.\n",
       "    return_dict (`bool`, *optional*):\n",
       "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
       "\n",
       "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
       "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
       "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
       "    \n",
       "Returns:\n",
       "    [`transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`] or a tuple of\n",
       "    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
       "    elements depending on the configuration ([`GPT2Config`]) and inputs.\n",
       "\n",
       "    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Language modeling loss (for next-token prediction).\n",
       "    - **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) -- Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
       "    - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
       "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
       "\n",
       "      Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
       "    - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "      sequence_length)`.\n",
       "\n",
       "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
       "      heads.\n",
       "    - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "      sequence_length)`.\n",
       "\n",
       "      Cross attentions weights after the attention softmax, used to compute the weighted average in the\n",
       "      cross-attention heads.\n",
       "    - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `torch.FloatTensor` tuples of length `config.n_layers`, with each tuple containing the cached key,\n",
       "      value states of the self-attention and the cross-attention layers if model is used in encoder-decoder\n",
       "      setting. Only relevant if `config.is_decoder = True`.\n",
       "\n",
       "      Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n",
       "      `past_key_values` input) to speed up sequential decoding.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       ">>> import torch\n",
       ">>> from transformers import AutoTokenizer, GPT2LMHeadModel\n",
       "\n",
       ">>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
       ">>> model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
       "\n",
       ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
       ">>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
       ">>> loss = outputs.loss\n",
       ">>> logits = outputs.logits\n",
       "```\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\conda\\envs\\pip310\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Convert the input_ids to a torch.tensor\n",
    "input_tensor = torch.tensor(input_ids)\n",
    "\n",
    "# Get the activation tensor from the layer 9 of the model \n",
    "# (the last layer)\n",
    "#activation_tensor = \n",
    "model.forward?\n",
    "outputs = model.forward(input_tensor)\n",
    "#[9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53b0cfab-cfe9-4df0-9757-bcb18741d7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__post_init__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'attentions',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hidden_states',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'logits',\n",
       " 'loss',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5549fa-2607-4f8f-b3c6-18220ac818fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aeb255-7ae0-47f6-8d68-6c403a7c9722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35aa76a6-a4a8-4cc2-b88a-75e66a7d300f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 50257])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e4c9a-fbc7-452f-bed5-258a8b60fb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb77702-2205-4627-865e-d933acce1fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b000153-8b17-43d7-8607-6f7e77f69aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0a8c2-9e4f-4440-a1f6-04e1d9aa811c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df34b1-ebb9-4220-8883-999c0cf8884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b98fb5-117d-418d-b5ab-60d1a75f36c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3af0bbd4-8fc7-4716-9e4b-16d5adfbf1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        CausalLMOutputWithCrossAttentions\n",
       "\u001b[1;31mString form:\u001b[0m CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[ -35.8890,  -35.2049,  -39.1336,  .. <...> 1]]]], grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)\n",
       "\u001b[1;31mLength:\u001b[0m      2\n",
       "\u001b[1;31mFile:\u001b[0m        d:\\conda\\envs\\pip310\\lib\\site-packages\\transformers\\modeling_outputs.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Base class for causal language model (or autoregressive) outputs.\n",
       "\n",
       "Args:\n",
       "    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n",
       "        Language modeling loss (for next-token prediction).\n",
       "    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
       "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
       "    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
       "        one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
       "\n",
       "        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
       "    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "        sequence_length)`.\n",
       "\n",
       "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
       "        heads.\n",
       "    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "        sequence_length)`.\n",
       "\n",
       "        Cross attentions weights after the attention softmax, used to compute the weighted average in the\n",
       "        cross-attention heads.\n",
       "    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
       "        Tuple of `torch.FloatTensor` tuples of length `config.n_layers`, with each tuple containing the cached key,\n",
       "        value states of the self-attention and the cross-attention layers if model is used in encoder-decoder\n",
       "        setting. Only relevant if `config.is_decoder = True`.\n",
       "\n",
       "        Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n",
       "        `past_key_values` input) to speed up sequential decoding."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ec57a-630b-423c-bb8e-4b5019a3996e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096605a3-c2d1-4e30-b537-3f6041e18411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f23b58-2a45-417a-8184-2620ea36a30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8010e7c-5812-417a-aa75-421894b180f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf96b3c-340d-425d-9034-df519bc4cd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d283163-65dd-448c-baf9-8eae9c8d96c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdc040-ab05-49c6-b6c2-821e71ae73dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906d3c4-1de3-44a4-9755-869cfe42344f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa45e93-8165-4d8c-b627-0383cc902c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212,\n",
       " 318,\n",
       " 262,\n",
       " 2420,\n",
       " 314,\n",
       " 561,\n",
       " 588,\n",
       " 284,\n",
       " 651,\n",
       " 262,\n",
       " 14916,\n",
       " 11192,\n",
       " 273,\n",
       " 329,\n",
       " 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865828d-3ff8-4428-b535-2b7230f2a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can then use the model's forward() method to get the activation tensor for the given text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
