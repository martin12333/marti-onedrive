{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c54fae4f-77e6-4c13-8471-fc7cfe483082",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (225919299.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[43], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    https://platform.openai.com/playground/p/JDgU4QisbgFa8ZhDIqkw0wP9\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://platform.openai.com/playground/p/JDgU4QisbgFa8ZhDIqkw0wP9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69fe8e1a-5fd8-4ef6-9769-39d92cef85d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_32100\\1796901329.py:16: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_32100\\1796901329.py:17: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c9e27-2874-4703-a786-84c269918f7d",
   "metadata": {},
   "source": [
    "\n",
    "I am editing a Python Jupyter notebook called \"Interactive Neuroscope\". It uses Huggingface Transformers.\n",
    "\n",
    "```\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "```\n",
    "\n",
    " (the loaded model is gpt-2-small )\n",
    "\n",
    "The author wrote:\n",
    "\n",
    "> This is an interactive accompaniment to neuroscope.io and to the studying learned language features post in 200 Concrete Open Problems in Mechanistic Interpretability\n",
    "\n",
    "My question is:\n",
    "\n",
    "how can I get a tensor with neuron activations ... e.g. from the layer 9, on a given text ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764fa46-e84c-41e7-9917-90e8403475f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5ecc3c-4ad5-4292-80f6-560d9d5414fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4e43b-c689-4f4a-b1d0-d8c50e2b6a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626d8016-319a-4fc7-84da-8d7f16ff9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and tokenizer\n",
    "name='gpt2'\n",
    "config = AutoConfig.from_pretrained(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForCausalLM.from_pretrained(name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "031af2a1-6441-4d2a-a634-478a1606cb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1212,   318,   262,  2420,   314,   561,   588,   284,   651,   262,\n",
       "         14916, 11192,   273,   329,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get the text you would like to generate the activation tensor for\n",
    "text = 'This is the text I would like to get the activation tensor for.'\n",
    "\n",
    "# Encode the text into tokens\n",
    "input_ids = tokenizer.encode(text)\n",
    "\n",
    "#\n",
    "#encoded_input = \n",
    "tokenizer(text, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c2770-daeb-444f-ab1c-132477a28dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797020b-ffe9-4204-b1d2-0edfa18b2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the input_ids to a torch.tensor\n",
    "input_tensor = torch.tensor(input_ids)\n",
    "\n",
    "# Get the activation tensor from the layer 9 of the model \n",
    "# (the last layer)\n",
    "#activation_tensor = \n",
    "model.forward?\n",
    "outputs = model.forward(input_tensor)\n",
    "#[9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b12a1d5-d215-408f-83b7-934564ea667d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        CausalLMOutputWithCrossAttentions\n",
       "\u001b[1;31mString form:\u001b[0m CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[ -35.8890,  -35.2049,  -39.1336,  .. <...> 1]]]], grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)\n",
       "\u001b[1;31mLength:\u001b[0m      2\n",
       "\u001b[1;31mFile:\u001b[0m        d:\\conda\\envs\\pip310\\lib\\site-packages\\transformers\\modeling_outputs.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Base class for causal language model (or autoregressive) outputs.\n",
       "\n",
       "Args:\n",
       "    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n",
       "        Language modeling loss (for next-token prediction).\n",
       "    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
       "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
       "    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
       "        one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
       "\n",
       "        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
       "    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "        sequence_length)`.\n",
       "\n",
       "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
       "        heads.\n",
       "    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "        sequence_length)`.\n",
       "\n",
       "        Cross attentions weights after the attention softmax, used to compute the weighted average in the\n",
       "        cross-attention heads.\n",
       "    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
       "        Tuple of `torch.FloatTensor` tuples of length `config.n_layers`, with each tuple containing the cached key,\n",
       "        value states of the self-attention and the cross-attention layers if model is used in encoder-decoder\n",
       "        setting. Only relevant if `config.is_decoder = True`.\n",
       "\n",
       "        Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n",
       "        `past_key_values` input) to speed up sequential decoding."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dcf06-d883-4bd1-a01b-ef4f05d58161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02ff2f-7fb5-4c44-beb3-6bc48e7f71e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1901fb-c715-49ee-afc6-e2637b108009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e169c1-a236-4708-a94b-3a623d43d999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665b54a-7980-4dd0-b471-617b4e3c87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71ce8f7a-efa8-4a51-99ed-913d2e21926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c552d1-fc3c-4de7-a6c0-d8d3bc934719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcf19d91-879a-43d2-82b3-29d57603d3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b646dde-b836-4d97-8cdd-4f92f746195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': Field(name='loss',type=typing.Optional[torch.FloatTensor],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000021158CD1C90>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'logits': Field(name='logits',type=<class 'torch.FloatTensor'>,default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000021158CD1C90>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'past_key_values': Field(name='past_key_values',type=typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000021158CD1C90>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'hidden_states': Field(name='hidden_states',type=typing.Optional[typing.Tuple[torch.FloatTensor]],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000021158CD1C90>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'attentions': Field(name='attentions',type=typing.Optional[typing.Tuple[torch.FloatTensor]],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000021158CD1C90>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " 'cross_attentions': Field(name='cross_attentions',type=typing.Optional[typing.Tuple[torch.FloatTensor]],default=None,default_factory=<dataclasses._MISSING_TYPE object at 0x0000021158CD1C90>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__dataclass_fields__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a433e86e-372c-490f-9ef0-8d9c106db596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_DataclassParams(init=True,repr=True,eq=True,order=False,unsafe_hash=False,frozen=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__dataclass_params__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9d61b-d078-4ce1-ac59-6c37cda56e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53b0cfab-cfe9-4df0-9757-bcb18741d7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__post_init__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'attentions',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hidden_states',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'logits',\n",
       " 'loss',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5549fa-2607-4f8f-b3c6-18220ac818fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aeb255-7ae0-47f6-8d68-6c403a7c9722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35aa76a6-a4a8-4cc2-b88a-75e66a7d300f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 50257])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e4c9a-fbc7-452f-bed5-258a8b60fb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb77702-2205-4627-865e-d933acce1fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b000153-8b17-43d7-8607-6f7e77f69aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0a8c2-9e4f-4440-a1f6-04e1d9aa811c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df34b1-ebb9-4220-8883-999c0cf8884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b98fb5-117d-418d-b5ab-60d1a75f36c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3af0bbd4-8fc7-4716-9e4b-16d5adfbf1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        CausalLMOutputWithCrossAttentions\n",
       "\u001b[1;31mString form:\u001b[0m CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[ -35.8890,  -35.2049,  -39.1336,  .. <...> 1]]]], grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)\n",
       "\u001b[1;31mLength:\u001b[0m      2\n",
       "\u001b[1;31mFile:\u001b[0m        d:\\conda\\envs\\pip310\\lib\\site-packages\\transformers\\modeling_outputs.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Base class for causal language model (or autoregressive) outputs.\n",
       "\n",
       "Args:\n",
       "    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n",
       "        Language modeling loss (for next-token prediction).\n",
       "    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
       "        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
       "    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
       "        one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
       "\n",
       "        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
       "    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "        sequence_length)`.\n",
       "\n",
       "        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
       "        heads.\n",
       "    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
       "        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "        sequence_length)`.\n",
       "\n",
       "        Cross attentions weights after the attention softmax, used to compute the weighted average in the\n",
       "        cross-attention heads.\n",
       "    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
       "        Tuple of `torch.FloatTensor` tuples of length `config.n_layers`, with each tuple containing the cached key,\n",
       "        value states of the self-attention and the cross-attention layers if model is used in encoder-decoder\n",
       "        setting. Only relevant if `config.is_decoder = True`.\n",
       "\n",
       "        Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n",
       "        `past_key_values` input) to speed up sequential decoding."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ec57a-630b-423c-bb8e-4b5019a3996e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096605a3-c2d1-4e30-b537-3f6041e18411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f23b58-2a45-417a-8184-2620ea36a30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8010e7c-5812-417a-aa75-421894b180f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf96b3c-340d-425d-9034-df519bc4cd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d283163-65dd-448c-baf9-8eae9c8d96c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdc040-ab05-49c6-b6c2-821e71ae73dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906d3c4-1de3-44a4-9755-869cfe42344f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa45e93-8165-4d8c-b627-0383cc902c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212,\n",
       " 318,\n",
       " 262,\n",
       " 2420,\n",
       " 314,\n",
       " 561,\n",
       " 588,\n",
       " 284,\n",
       " 651,\n",
       " 262,\n",
       " 14916,\n",
       " 11192,\n",
       " 273,\n",
       " 329,\n",
       " 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865828d-3ff8-4428-b535-2b7230f2a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can then use the model's forward() method to get the activation tensor for the given text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
