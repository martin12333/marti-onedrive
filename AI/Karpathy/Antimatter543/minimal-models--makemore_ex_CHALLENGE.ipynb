{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm ... small edits by M. M.\n",
    "# (#aaa #aaaa #aaaa ... \"attention\", \"importance\" marks, just for M. M.)\n",
    "#mm-todo-tweak #aaaa\n",
    "\n",
    "#mm\n",
    "import torch\n",
    "torch.set_printoptions(profile='short')\n",
    "#%precision 2\n",
    "# https://pytorch.org/docs/stable/generated/torch.set_printoptions.html\n",
    "#torch.set_printoptions(precision=0)\n",
    "torch.set_printoptions(precision=4)\n",
    "torch.set_printoptions(threshold=7)\n",
    "torch.set_printoptions(edgeitems=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters!!! CHANGE STUFF HERE\n",
    "#mm-todo-tweak #aaaa\n",
    "block_size = 5 #4 # 1 # 2 # 3 # num of past chars to use to predict next char\n",
    "embedding_size = 32 # 16 # 6 # 2 # 10 #4 # 5 # 2 #10\n",
    "num_neurons = 256 #  100  # 250  # 12 # 24 # 50 # 100 # 250 # for layer 1 \n",
    "batch_size =  1024 # 256 # 128 # 48 # 32 # 48\n",
    "\n",
    "n200k= 20000 # 1000* 20 # 10 # 80 # 40 # 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy has CHALLENGED US TO BEAT HIS VALIDATION LOSS OF 2.2 (dev set loss).\n",
    "WE MUST!!! ONWARDS!!!! (We'll change the hyperparameters hbehehe)\n",
    "Also, he said we should be able to read a good chunk of the paper by now, so let's try that too :D\n",
    "\n",
    "Anyways, for this, let's make nice functions where we can change all the hyperparameters.\n",
    "Batch size, # neurons, C embedding size, lr, ... \n",
    "\n",
    "# E01: Beat Big K's 2.2 validation (dev set) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read words and generate our char <-> int functions\n",
    "words = open('../names.txt', 'r').read().splitlines() # Read everything\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # string to integer\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()} # int to string\n",
    "\n",
    "words[:8] # woo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making our dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(words, block_size, doprint=False):\n",
    "\t\"\"\"returns X, Y datasets based on words list\n",
    "\tblock_size: context length: how many characters do we take to predict the next one?\n",
    "\t\"\"\"\n",
    "\tX, Y = [], []\n",
    "\n",
    "\tfor w in words:\n",
    "\t\tif doprint:\n",
    "\t\t\tprint(w)\t\n",
    "\t\tcontext = [0] * block_size # this would make [0, 0, ...] based on block_size\n",
    "\n",
    "\t\tfor ch in w + '.': \n",
    "\t\t\tix = stoi[ch] # index of char\n",
    "\t\t\tX.append(context)\n",
    "\t\t\tY.append(ix)\n",
    "\t\t\t\n",
    "\t\t\tif doprint:\n",
    "\t\t\t\tprint(''.join(itos[i] for i in context), '------>', itos[ix])\n",
    "\t\t\tcontext = context[1:] + [ix] # [0,0,0] -> [0,0, ix] like a rolling effect\n",
    "\n",
    "\tX = torch.tensor(X)\n",
    "\tY = torch.tensor(Y)\n",
    "\tprint(X.shape, Y.shape)\n",
    "\treturn X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 32033\n",
      "torch.Size([182625, 5]) torch.Size([182625])\n",
      "torch.Size([22655, 5]) torch.Size([22655])\n",
      "torch.Size([22866, 5]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# Generate our train, dev, and test loss. We'll be trying to get dev loss < 2.2\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "print(f'number of words: {len(words)}')\n",
    "\n",
    "Xtr, Ytr = generate_dataset(words[:n1], block_size)\n",
    "Xdev, Ydev = generate_dataset(words[n1:n2], block_size)\n",
    "Xte, Yte = generate_dataset(words[n2:], block_size)\n",
    "# Of course, the sizes are way more massive than # word examples because these are the examples from the words (char stuff), not the words themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, initialisation of our model! I.e, we'll initialise the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in total: 49019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 32]), torch.Size([160, 256]))"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # This will be a 'global' generator\n",
    "def generate_parameters(block_size, embedding_size, num_neurons):\n",
    "\t# We booosting our hidden layer size\n",
    "\tC = torch.randn(27, embedding_size, generator=g) # Generate embedding of characters, e.g (27,10)\n",
    "\n",
    "\tW1 = torch.randn((embedding_size * block_size, num_neurons), generator=g) # first weights takes input from \n",
    "\tb1 = torch.randn(num_neurons, generator=g)\n",
    "\tW2 = torch.randn((num_neurons, 27), generator=g)\n",
    "\tb2 = torch.randn(27, generator=g)\n",
    "\tparameters = [C, W1, b1, W2, b2] # for easy summing parameters\n",
    "\n",
    "\tfor p in parameters:  # Turn on requires grad for our parameter matrices\n",
    "\t\tp.requires_grad = True\n",
    "\t\t\n",
    "\tprint(\"Number of parameters in total:\", sum(p.nelement() for p in parameters)) # num of parameters in total\n",
    "\n",
    "\treturn parameters\n",
    "parameters = generate_parameters(block_size, embedding_size, num_neurons)\n",
    "C, W1, b1, W2, b2 = parameters\n",
    "C.shape, W1.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a consolidation bit, we'd be wanting to do this to push it through the first layer\n",
    "```\n",
    "C[Xtr] @ W1 + b1 # -> doesn't work, as C[Xtr].shape -> torch.Size([182484, 3, 10]) || Each example has 3 inputs, and each char input has an embedding size of 10.\n",
    "W1 -> torch.Size([30, 200]). \n",
    "```\n",
    "We want to flatten the embedded stuff so each example just reads as a 30 length vector (or w.e the size of the thingy is), i.e ([# examples, block*embedding sizes ]). We do this via: \n",
    "```\n",
    "C[Xtr].view((-1, block_size*embedding_size)).shape -> torch.Size([182484, 30])\n",
    "C[Xtr].view((-1, block_size*embedding_size)) @ W1 + b1 # works! -> torch.Size([182484, 200]). Yay!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we'll have to do gradient descent on the model. \n",
    "\n",
    "(this is called the training loop or whatever)\n",
    "If I remember well,\n",
    "\n",
    "0th step; we can minibatch! To make the mfer run faster, even at some loss of accuracy of the gradient to descend, it's still much better overall.\n",
    "\n",
    "#### First step, is forward passing.\n",
    " We get the training data (specifically, training data from minibatch). We embed it, and flatten that embedding (using the efficient .view(..)), and then 'run it through' the layers by doing our activation on it. Then we run it through the second layer (our final one), giving us the logits.\n",
    "\n",
    "We can get the loss straight up by doing F.cross_entropy, or we can get the probability dist. by softmaxing it. (And then you could work out loss. Remember, these functions are doing very simple things we've already done!! -- they just exp, normalise weights, and then get the negative loss likelihood).\n",
    "\n",
    "#### The backwards pass\n",
    "Make sure to set all our grads to 0.\n",
    "Then we just  loss.backward(), propagating through every operation done and getting it's gradient w.r.t loss\n",
    "\n",
    "#### Updating weights\n",
    "And we just update the weights now :D \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm\n",
    "#aaaa\n",
    "# add embedding projector data to summary\n",
    "##writer.add_embedding(P.transpose(1,0)   , metadata= ['@']+chars )\n",
    "                    # ,     global_step=n27)\n",
    "\n",
    "#writer.flush()\n",
    "## close writer\n",
    "##writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented hardcode first\n",
    "\n",
    "def train(X: torch.Tensor, Y: torch.Tensor, num_steps: int, block_size: int, embedding_size: int, batch_size:int,  parameters, doTrack = False): \n",
    "\t\"\"\"_summary_\n",
    "\n",
    "\tArgs:\n",
    "\t\tX (torch.Tensor): X dataset to train on (train with our Xtr!)\n",
    "\t\tY (torch.Tensor): Y dataset for labels\n",
    "\t\tnum_steps (int): Number of times to go through loop\n",
    "\t\tblock_size (int): Context length; # characters used to predict next one\n",
    "\t\tembedding_size (int): Size of embedding for the characters\n",
    "\t\tparameters (_type_): param\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\tC, W1, b1, W2, b2 = parameters\n",
    "\t# track stats (optional)\n",
    "\tstepi, lossi = [], []\n",
    "\n",
    "\t# Main training loop\n",
    "\tfor i in range(num_steps):\n",
    "        \n",
    "#mm #aaaa generator=        \n",
    "\t\tix = torch.randint(0, X.shape[0], (32,), generator=g) # We get {minibatch_size} random ints from size of 0- training set\n",
    "\t\t\n",
    "\t\t# forward pass -- on our minibatch :o !!!!\n",
    "\t\temb = C[X[ix]] # (32,3,2) here || (minibatch_size, block_size, embed size))\n",
    "\t\t# print(f'{emb.shape=}')\n",
    "\t\th = torch.tanh(emb.view(-1, block_size*embedding_size) @ W1 + b1) # We do the whole embed flatten, push it through layer 1, and tanh activation on it. All elements of h is between [-1, 1]\n",
    "\t\tlogits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "\n",
    "\t\tloss = F.cross_entropy(logits, Y[ix]) # Does the epic normalisation stuff for us \n",
    "\t\t# print(f'Loss during this minibatch loop: {loss=}')\n",
    "\n",
    "\t\t# backward pass - zero grad, backprop\n",
    "\t\tfor p in parameters:\n",
    "\t\t\tp.grad = None\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\t# update\n",
    "\t\t# lr = lrs[i]\n",
    "        \n",
    "#mm-todo-tweak #aaaa\n",
    "       \n",
    "\t\tlr = 0.1 if i < 10000 else 0.01\n",
    "\t\t#lr = 0.01 if i < 100000 else 0.01\n",
    "\t\t#lr = 0.5 if i < 100000 else 0.01\n",
    "\t\t#lr = 0.2 if i < 100000 else 0.03\n",
    "\t\tfor p in parameters:\n",
    "\t\t\tp.data += -lr * p.grad\n",
    "            \n",
    "#mm\n",
    "\t\tif i % 1000 == 0: # print every once in a while\n",
    "\t\t\tprint(f'{i:7d}/{num_steps:7d}: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "\t\tstepi.append(i)\n",
    "\t\tlossi.append(loss.log10().item())            \n",
    "\t\t##lossi.append(loss.item())            \n",
    "            \n",
    "\t\t# track stats (optional part)\n",
    "\t\tif doTrack:\n",
    "\t\t\t#writer.add_scalar(  \"loss3\",  loss.log10().item(),i)  #mm\n",
    "\t\t\twriter.add_scalar(  \"loss10\",  loss.item(),i)  #mm\n",
    "\n",
    "\treturn loss, [stepi, lossi] # Not needed, I'm just putting it here because why not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  20000: 31.0934\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"d:/ai/makemore_ex_CHALLENGE\")\n",
    "\n",
    "#mm\n",
    "\n",
    "#train_loss, track_stats = train(Xtr, Ytr, 2*10**5, block_size, embedding_size, batch_size, parameters, doTrack=False) \n",
    "train_loss, track_stats = train(Xtr, Ytr, n200k, block_size, embedding_size, batch_size, parameters, doTrack=False)  #True) \n",
    "print(train_loss)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(track_stats[0], track_stats[1]) # stepi, lossi\n",
    "#plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! Now let's evaluate our model!\n",
    "(Our model is our parameters :ooooo)\n",
    "It also comes to my attention now that maybe setting the hyperparametrs as constants  would be better so I didn't have to feed in every goddamn thing into every function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(X, Y, block_size, embedding_size, parameters):\n",
    "\tC, W1, b1, W2, b2 = parameters\n",
    "\temb = C[X] # (X.shape[0],3,2) here -- whole set in this case\n",
    "\th = torch.tanh(emb.view(-1, block_size*embedding_size) @ W1 + b1) # (blah, 100)\n",
    "\n",
    "\tlogits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "\tloss = F.cross_entropy(logits, Y) # Does the epic normalisation stuff\n",
    "\tprint(f'{loss=}')\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = evaluate_loss(Xtr, Ytr, block_size, embedding_size, parameters)\n",
    "dev_loss = evaluate_loss(Xdev, Ydev, block_size, embedding_size, parameters)  ### YOOOOO 2.18 POOOOOG\n",
    "test_loss = evaluate_loss(Xte, Yte, block_size, embedding_size, parameters) ## POGGING RN\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "loss=tensor(2.3137, grad_fn=<NllLossBackward0>)\n",
    "loss=tensor(2.3565, grad_fn=<NllLossBackward0>)\n",
    "loss=tensor(2.3614, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "loss=tensor(2.4311, grad_fn=<NllLossBackward0>)\n",
    "loss=tensor(2.4705, grad_fn=<NllLossBackward0>)\n",
    "loss=tensor(2.4727, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "loss=tensor(2.4665, grad_fn=<NllLossBackward0>)\n",
    "loss=tensor(2.5204, grad_fn=<NllLossBackward0>)\n",
    "loss=tensor(2.5233, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "loss=tensor(2.4705, grad_fn=<NllLossBackward0>)\n",
    "loss=tensor(2.5227, grad_fn=<NllLossBackward0>)\n",
    "loss=tensor(2.5162, grad_fn=<NllLossBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAp4AAAKTCAYAAACw6AhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABYlUlEQVR4nO3dfXxU5b3v/e+aIUTyQEgIUQkhE4T6FEMsIYp1U7WVqq3FbprWeu9dba319mj3AXrwoXtXpae2Cu5id+uxutsq+4E7EjlKt9UKdatUCwZipyGCApIJMSAhJOQJTYY16/4DExMyM5kkM2vWZD7v16uvl1mzZs1vWNJ8va51XT/DsixLAAAAQIy54l0AAAAAkgPBEwAAALYgeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGwxId4FhBMIBHTw4EFlZmbKMIx4lwMAAIBTWJalzs5OTZ8+XS5X+DFNRwfPgwcPqqCgIN5lAAAAYBiNjY2aMWNG2HMcHTwzMzMlnfwikydPjnM19vP7/dq0aZMWLVqklJSUeJeDU3B/nI3741zcG2fj/jiXU+9NR0eHCgoK+nNbOI4Onn3T65MnT07a4JmWlqbJkyc76l8wnMT9cTbuj3Nxb5yN++NcTr83kTwWyeIiAAAA2ILgCQAAAFvENHg+9thjKikp6Z8qX7BggV588cVYfiQAAAAcKqbBc8aMGXrwwQdVU1OjHTt26IorrtDixYv19ttvx/JjAQAA4EAxXVx07bXXDvr5gQce0GOPPaZt27bp/PPPj+VHAwAAwGFsW9VumqaqqqrU3d2tBQsWBD2np6dHPT09/T93dHRIOrmKy+/321Knk/R952T87omA++Ns3B/n4t44G/fHuZx6b0ZSj2FZlhXDWrRz504tWLBAH330kTIyMrRu3Tpdc801Qc+9//77tXLlyiHH161bp7S0tFiWCQAAgFE4fvy4brjhBrW3tw+7/WXMg2dvb68OHDig9vZ2PfPMM/r1r3+t1157Teedd96Qc4ONeBYUFKilpSVp9/HcvHmzrrzySkfu15XsuD/Oxv1xLu6Ns3F/nMup96ajo0O5ubkRBc+YT7VPnDhRs2fPliTNmzdP27dv189//nM9/vjjQ85NTU1VamrqkOMpKSmO+gO2W7J/f6fj/jgb98e5uDfOxv1xLqfdm5HUYvs+noFAYNCoJgAAAJJDTEc877nnHl199dWaOXOmOjs7tW7dOr366qt66aWXYvmxAAAAcKCYBs/m5mZ985vf1KFDh5SVlaWSkhK99NJLuvLKK2P5sQAAAHCgmAbP3/zmN7G8PAAAABIIvdoBAABgC4InAIwzgUBMd8kDgFGzrXMRACA26praVbWjUdW+Vu1r7pLftJTiNjQ7L0PlnhxVlBWoOD8r3mUCAMETABKVr6Vbd26oVXV9q9wuQ+aAkU6/aWn3oU7tOdyltVsbVF6Uo1VLSuTJTY9jxQCSHVPtAJCANnqbtGjNFtU0tEnSoNA5UN/xmoY2LVqzRRu9TbbVCACnYsQTABLMRm+TllZ6NZInOc2AJVOWllZ6JUmLS/NjUhsAhMOIJwAkkPqWbq2oqh1R6BzIkrSiqla+lu5olgUAESF4AkACuWtDrUxrbKvWTcvSnRtqo1QRAESO4AkACWLn++2qrm8N+TxnpMyAper6VtU1tUepMgCIDM94AkCCeKamURNchk6ECJ6f/dQ03XHFbJ19eqbMgKW3DrRp5X/t0oHW40POdbsMVe1oZJslALZixBMAEkS1rzVk6JSkSRPd+vWf6nXtL1/X//PrNxWwpMf/fp4MY+i5ZsDSdl9bDKsFgKEY8QSABLGvuSvs63+o+2DQz3c+81f95d5FmpOXoT2Hh753b3NnVOsDgOEQPAEgAQQClvxm+Gc7PVPTtPzKT6m0IFvZ6SlyfTzUOX3KpKDB029aCgQsuVxBhkQBIAYIngCQAFwuQyluI2z4/M2N89V07EPd/X9rdbijRy5D2rz8s5roDv5UVYrbIHQCsBXPeAJAgpidlxHytSlpKTorL0O/+O+9+vN7R/XekS5lTUoJe705eZnRLhEAwmLEEwASRLknR3sOdwXdTqn9Q79au3v1jfKZau7s0fQpk3TXVeeEvJbbZWi+JzuW5QLAEIx4AkCCqCgrCLmHp2VJ3/v/3tIF+VnatHSh7v3SefrpC7tDXssMWKooK4hVqQAQFCOeAJAgivOzVF6Uo5qGtqAB9I19R3Xlmi2Djnnu/v2Q89wuQ/MKs9nDE4DtGPEEgASyakmJ3ME25hwBt2Fo1ZKSKFUEAJEjeAJAAvHkpmt1RYlGGz0NSasrSuTJTY9mWQAQEabaASDBLC7NlyStqKqVaVkR9W53uwy5DUOrK0r63w8AdmPEEwAS0OLSfG1atlDzCk+uTHeH2I+z73hZYbY2LVtI6AQQV4x4AkCC8uSma/2tC1TX1K6qHY3a7mvT3uZO+U1LKW5Dc/IyNd+TrYqyAhYSAXAEgicAJLji/KxBwZI2mACciql2ABhnCJ0AnIrgCQAAAFsQPAEAAGALgicAAABsQfAEAACALQieAAAAsAXBEwAAALYgeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGxB8AQAAIAtCJ4AAACwBcETAAAAtiB4AgAAwBYETwAAANiC4AkAAABbEDwBAABgC4InAAAAbEHwBAAAgC0IngAAALAFwRMAAAC2IHgCAADAFgRPAAAA2ILgCQAAAFsQPAEAAGALgicAAABsQfAEAACALQieAAAAsAXBEwAAALYgeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGxB8AQAAIAtCJ4AAACwBcETAAAAtiB4AgAAwBYETwAAANiC4AkAAABbEDwBAABgC4InAAAAbEHwBAAAgC0IngAAALAFwRMAAAC2IHgCAADAFgRPAAAA2ILgCQAAAFsQPAEAAGALgucpAgEr3iUAAACMSxPiXUC81TW1q2pHo6p9rdrX3CW/aSnFbWh2XobKPTmqKCtQcX5WvMsEAABIeEkbPH0t3bpzQ62q61vldhkyB4x0+k1Luw91as/hLq3d2qDyohytWlIiT256HCsGAABIbEk51b7R26RFa7aopqFNkgaFzoH6jtc0tGnRmi3a6G2yrUYAAIDxJulGPDd6m7S00quRPMlpBiyZsrS00itJWlyaH5PaAAAAxrOkGvGsb+nWiqraEYXOgSxJK6pq5WvpjmZZAAAASSGpguddG2plWmNbtW5alu7cUBuligAAAJJH0gTPne+3q7q+NeTznJEyA5aq61tV19QepcoAAACSQ9I84/lMTaMmuAydCBE8DUO67bNn6RvlMzUtM1X1Ld36l5f36sW6D4ac63YZqtrRyDZLAAAAI5A0wbPa1xoydErS/7hstr5yYb7+8dmdqj/arYuKpuqRr5eqtbtab9a3DjrXDFja7muLdckAAADjStIEz33NXSFfm+h26fbLz9Lf/fpNvXXgmCSpsfV9lXmydcNFM4cET0na29wZq1IBAADGpaQInoGAJb8ZerSzcGqa0iZO0L/ffNGg4ylul3YdDP4sp9+0FAhYcrmMqNYKAAAwXiVF8HS5DKW4jZDhMz315B/Dt5/arg86Phr0Wu+JQND3pLgNQicAAMAIJEXwlKTZeRnafSj49Pjew53q8ZuaPmVS0Gn1YObkZUazPAAAgHEvaYJnuSdHew53Bd1OqbvX1BN/2q8ffuk8uQxpu69NmadNUJknR10f+bXhrcGtMt0uQ/M92XaVDgAAMC4kTfCsKCvQ2q0NIV//50171Nrdq/9x2WwV5KSp4yO/3m5q16OvvjfkXDNgqaKsIJblAgAAjDtJEzyL87NUXpSjmoa2kJvIP/mGT0++4Qt7HbfL0LzCbPbwBAAAGKGk6VwkSauWlMhtjG1BkNswtGpJSZQqAgAASB5JFTw9uelaXVGi0UZPQ9LqihJ5ctOjWRYAAEBSSJqp9j6LS/MlSSuqamVaVkS9290uQ27D0OqKkv73AwAAYGSSasSzz+LSfG1atlDzCk+uTHeH2I+z73hZYbY2LVtI6AQAABiDpBvx7OPJTdf6WxeorqldVTsatd3Xpr3NnfKbllLchubkZWq+J1sVZQUsJAIAAIiCpA2efYrzswYFS9pgAgAAxEZSTrWHQ+gEAACIDYInAAAAbEHwBAAAgC0IngAAALAFwRMAAAC2IHgCAADAFgRPAAAA2ILgCQAAAFsQPAEAAGCLmAbPn/70p5o/f74yMzOVl5en6667Tu+++24sPxIAAAAOFdPg+dprr+n222/Xtm3btHnzZvn9fi1atEjd3d2x/FgAAAA4UEx7tf/hD38Y9PNTTz2lvLw81dTUaOHChbH8aAAAADhMTIPnqdrb2yVJOTk5QV/v6elRT09P/88dHR2SJL/fL7/fH/sCHabvOyfjd08E3B9n4/44F/fG2bg/zuXUezOSegzLsqwY1tIvEAjoy1/+so4dO6bXX3896Dn333+/Vq5cOeT4unXrlJaWFusSAQAAMELHjx/XDTfcoPb2dk2ePDnsubYFz9tuu00vvviiXn/9dc2YMSPoOcFGPAsKCtTS0jLsFxmP/H6/Nm/erCuvvFIpKSnxLgen4P44G/fHubg3zsb9cS6n3puOjg7l5uZGFDxtmWq/44479Pzzz2vLli0hQ6ckpaamKjU1dcjxlJQUR/0B2y3Zv7/TcX+cjfvjXNwbZ+P+OJfT7s1Iaolp8LQsS9/73vf07LPP6tVXX1VRUVEsPw4AAAAOFtPgefvtt2vdunXauHGjMjMz9cEHH0iSsrKyNGnSpFh+NAAAABwmpvt4PvbYY2pvb9dll12mM888s/9/Tz/9dCw/FgAAAA4U86l2AAAAQKJXOwAAAGxC8AQAAIAtCJ4AAACwBcETAAAAtiB4AgAAwBYETwAAANiC4AkAAABbEDwBAABgC4InAAAAbEHwBAAAgC0IngAAALAFwRMAAAC2IHgCAADAFgRPAAAA2ILgCQAAAFsQPAEAAGALgicAAABsQfAEAACALQieAAAAsAXBE0DCCASseJcAABiDCfEuAABCqWtqV9WORlX7WrWvuUt+01KK29DsvAyVe3JUUVag4vyseJcJAIgQwROA4/haunXnhlpV17fK7TJkDhjp9JuWdh/q1J7DXVq7tUHlRTlataREntz0OFYMAIgEU+0AHGWjt0mL1mxRTUObJA0KnQP1Ha9paNOiNVu00dtkW40AgNFhxBOAY2z0NmlppVcjeZLTDFgyZWlppVeStLg0Pya1AQDGjhFPAI5Q39KtFVW1YUPnT75ygbz3Xinfg1/UeWdOHvSaJWlFVa18Ld0xrRMAMHoETwCOcNeGWplW6Nh52aem6avzZujbT+3Q/B//Ue8e7hxyjmlZunNDbSzLBACMAcETQNztfL9d1fWtIZ/nlKSZU9PU3PmR3jrQpiNdPUHPNQOWqutbVdfUHstyAQCjRPAEEHfP1DRqgssI+frDFSX60eJizchOk+/BL+r1uy4Pea7bZahqR2MsygQAjBGLiwDEXbWvVSfCjHau/N0uNRw9rm+Uz9TiX74RdkreDFja7muLRZkAgDEieAKIu33NXWFf7+w5oe6eEwpYlo509Qx7vb3NQ5//BADEH1PtAOIqELDkN6PbCtNvWrTXBAAHIngCiCuXy1CKO/TznaOR4jbkCvPMKAAgPgieAOJudl5GVK83Jy8zqtcDAEQHwXOcYXoRiajckyN3lEYo3S5D8z3ZUbkWACC6WFyU4Oqa2lW1o1HVvlbta+6S37SU4jY0Oy9D5Z4cVZQVqDg/K95lAmFVlBVo7daGqFzLDFiqKCuIyrUAANFF8ExQvpZu3bmhVtX1rXK7jEGbaftNS7sPdWrP4S6t3dqg8qIcrVpSIk9uehwrBkIrzs9SeVGOahraQm4i/9s3fPrtG76w13G7DM0rzFZxfpb8fn8MKgUAjAVT7Qloo7dJi9ZsUU3Dyb0KQ/2i7jte09CmRWu2aKO3ybYagZFataREbmNs0+1uw9CqJSVRqggAEG0EzwSz0dukpZVe9ZqBsO0FBzIDlnrNgJZWegmfcCxPbrpWV5RotNHTkLS6gpF9AHAygmcCqW/p1oqqWp0aNyu/e7Hu/dJ5w77fkrSiqla+lu6Y1AeM1eLSfD1yfakmul0RLzZyuwxNdLv0yPWlWlyaH+MKAQBjQfBMIHdtqA3bKjASpmXpzg21UaoIiL7FpfnatGyh5hWeXJkeKoD2HS8rzNamZQsJnQCQAFhclCB2vt+u6vrWMV/HDFiqrm9VXVM7q93hWJ7cdK2/dUH/rg3bfW3a29zZv2vDnLxMzfdks2sDACQYgmeCeKamURNchk6EeK7TMKS7rz5H188vkN8M6D/fPKBH/rg36Llul6GqHY2O/YUdCFh0nYGkk6vdB/57yr8bAJDYCJ4JotrXGjJ0StKSeTP0mz/V67pH39CnC7P18FfnaoevTa/vaxlyrhmwtN3XFstyR4S9SBEpQicAJDaCZ4LY19wV9vV3DnXq5y+fHOH0HT2uby7w6DOzpwYNnpK0t7kz6jWOFHuRAgCQXFhclAACAUt+M/yionc+6Bj085HOjzQ1IzXk+X7Timt7TfYiBQAg+TDimQBcLkMpbiNs+DxxymuWJYWblUxxG3Gbtuzbi3QksdcMWDJlaWmlV5JYwQwAQAJixDNBzM7LiOr15uRlRvV6kQq1F+lA4fYlZS9SAAASF8EzQZR7ciLeUHs4bpeh+Z7sqFxrpCLZi/TWf6/RP296N+Tr7EUKAEBiIngmiIqygohbZA7HDFiqKCuIyrVGom8v0uG+R/uHfnX3miFfH7gXKQAASBw845kgivOzVF6Uo5qGtiHB7fontg05/7v/XhP0Om6XoXmF2XHZnmi4vUj7VH73Yu062KEfPb8r5DlO34sUAAAMxYhnAlm1pERuY2zT7W7D0KolJVGqaGSG24t0JJy2FykAABgewTOBeHLTtbqiRKONnoak1RXx2wtzuL1IR8oJe5ECAIDIMdWeYPq2EVpRdXKRTiTPfbpdhtyGodUVJXHbhiiSvUhHqm8vUrrZAACQGBjxTECLS/O1adlCzSs8uTI91Gr3vuNlhdnatGxhXPe+7NuLNJriuRcpAAAYOUY8E5QnN13rb13Q3+d8u69Ne5s7+/ucz8nL1HxPtqP6nM/Oy9DuQ9GbHo/XXqQAAGB0CJ4Jrjg/a1CwdPLUc7knR3sOd0VlW6h47kUKAABGh6n2ccapoVMaH3uRAgCA0WPEE7YJtxfpQMH2JR0onnuRAgCA0WPEE7ZK9L1IAQDA6BE8YatE34s0mQSi9FgEAAB9mGqH7RJ1L9Lxrm+HhGpfq/Y1d/XvkDA7L0PlnhxH7ZAAAEhMBE/ExeLSfM2dMUV3bqhVdX2r3C4jaADtO15WmK2HljDSGQu+lu6Q98FvWtp9qFN7Dndp7dYGlRflaBX3AQAwSgRPxE0i7kU63mz0NvWPPEsKOfrcd7ymoU2L1mxh5BkAMCoET8RdIu1FOp5s9DZpaaVXI3mS0wxYMmVpaaVXkgifAIARYXERHIfQGXv1Ld1aUVU7otA5kKWTz+j6WrqjWRYAYJwjeAJJ6K4Nn0yvj5ZpWbpzQ22UKgIAJAOCJ5Bkdr7frur61jF3kTIDlqrrW1XX1B6lygAA4x3PeAJJ5pmaRk1wGToRInheXXyG/ufn58gzNV0f9pp6+2CHbvm3HfrQbw451+0yVLWjkcVfAICIEDyBJFPtaw0ZOqdlpupfvnGhHnzxHb309gdKnzhB84tyFKrZlBmwtN3XFsNqAQDjCcETSDL7mrtCvpaXmaoUt0t/qPtATcc+lCS9e7gz7PX2Nod/HQCAPgRPIIkEApb8ZuhnO3cf6tDre1v0h6V/oy17WvSnvUf0Qt0hdXx4IuR7/KbFFlgAgIiwuAhIIi6XoRR36IAYsKS/+82buunJ7drX3KkbL/Hov79/mWZkTwr5nhS3QegEAESE4Akkmdl5GcOeU9PQpjV/3Ksv/suf5DcD+sL5Z4Q8d05eZjTLAwCMY0y1A0mm3JOjPYe7gm6nVFowRZecNVV/2tuio109Kp05RTnpE/VeiOdC3S5D8z3ZsS4ZADBOEDyBJFNRVqC1WxuCvtb50QldVJSjb19apMzUCXr/2Id64Pe79eqeI0HPNwOWKsoKYlkuAGAcIXgCSaY4P0vlRTmqaWgbMur53pEu3fjk9oiu43YZmleYzR6eAICI8YwnkIRWLSmRO9TmnBFyG4ZWLSmJUkUAgGRA8ASSkCc3XasrSjTa6GlIWl1RIk9uejTLAgCMc0y1A0lqcWm+JGlFVa1My4qod7vbZchtGFpdUdL/fgAAIsWIJ5DEFpfma9OyhZpXeHJlujvEfpx9x8sKs7Vp2UJCJwBgVBjxBJKcJzdd629doLqmdlXtaNR2X5v2NnfKb1pKcRuak5ep+Z5sVZQVsJAIADAmBE8Akk6udh8YLGmDCQCINqbaAQRF6AQARBvBEwAAALYgeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGxB8AQwLgQiaPkJAIgvNpAHkJD6Oi1V+1q1r7mrv9PS7LwMlXtytOTCM+NdIgDgFARPAAnF19KtOzfUqrq+VW6XIXPASKfftLT7UKf2HO5SZbVPq8qlA0eP66wzaPUJAE7AVDuAhLHR26RFa7aopqFNkgaFzoEGHr/u0Te00dtkS30AgPAY8QSQEDZ6m7S00quRPsnZGwhoaaVXkrS4ND/qdQEAIseIJwDHq2/p1oqq2pChs/K7F+veL50X8v2WpBVVtfK1dMekPgBAZAieABzvrg21Mq2xrVo3LUt3bqiNUkUAgNEgeAJwtJ3vt6u6vjXk85yRMgOWqutbVdfUHqXKAAAjRfAE4GjP1DRqgsuIyrXcLkNVOxqjci0AwMgRPAE4WrWvVSeitDm8GbC03dcWlWsBAEaO4AnA0fY1d0X1enubO6N6PQBA5AieABwrELDkN6PbCtNvWrTXBIA4IXgCMUbIGT2Xy1CKOzrPd/ZJcRtyRemZUQDAyLCBPBBlw/UQrygrUHE+LRwjNTsvQ7sPRW96fE5eZtSuBQAYGYInECWR9hBfu7VB5UU5WrWkRJ7c9DhWnBjKPTnac7hrzNspSSdXtc/3ZEehKgDAaDDVDkTBSHuI1zS0adGaLfQQj0BFWUFUQqd08s+/oqwgKtcCAIwcI57AGL2w85CWrt85oh7iZsCSKYse4hEozs9SeVGOahraQgbQ65/YNux13C5D8wqzecwBAOIopiOeW7Zs0bXXXqvp06fLMAw999xzsfw4IC7+6dm6EYXOgeghHplVS0rkNsa2IMhtGFq1pCRKFQEARiOmwbO7u1tz587Vo48+GsuPAeLKHHXs/Pj99BAflic3XasrSjTa6GlIWl3BM7UAEG8xnWq/+uqrdfXVV8fyI4C42XWwQ1Lfc5ufRKK//XS+fvjF83TRT15WrxnoP/7E389TV88JLV//10HXGdhDnGng0PoeR1hRVSvTsiJ+7nOiy6VVFXN5nAEAHMBRz3j29PSop6en/+eOjpO/2P1+v/x+f7zKipu+75yM3z0R/M7bqBJJqa7BAeiPbx/U/deer6uL8/SHukOSpJz0ibr8nDzd/OSbSnUPDUxul6ENOxp0dt65dpSesK45P0/FZ1yiH/6uTjUNbUN2D+jjdhma8PFI9IZbL1LR6ZP5e+Qg/H+bs3F/nMup92Yk9RiWZdmyu7VhGHr22Wd13XXXhTzn/vvv18qVK4ccX7dundLS0mJYHRBdJSUlSktL07ZtJxe9nHXWWSoqKtIf//jHOFcGAEB0HT9+XDfccIPa29s1efLksOc6KngGG/EsKChQS0vLsF9kPPL7/dq8ebOuvPJKpaSkxLscnOLiH7+kez9t6oc7XOoJDH768NwzJ+uZ2y7V5Q//t5o7PtLvvrdQf6g7pP/zyt6Q10txGfrLvYtiXfa4FQhYgzoS8ffHubg3zsb9cS6n3puOjg7l5uZGFDwdNdWempqq1NTUIcdTUlIc9Qdst2T//k4UCFjqOnHyn3sChnrMwcHT+36ndh/q1LVzZ2jL3iOanZepyu3bh5w3UI8pud0TaOcYZfz9cS7ujbNxf5zLafdmJLU4KngCicLlMpQyTEB8evsBfevSIp0++TS9sa9Fh9o/Cns+PcQHO3UEEwCQ+GIaPLu6urRv377+n+vr6+X1epWTk6OZM2fG8qOBmJs1LUPSsZCvb/Qe1A++eK6uLy/Q909ZyR5MsvcQp8c9AIx/MQ2eO3bs0OWXX97/8/LlyyVJN954o5566qlYfjQQc/MKsxUueHb2nNCLdR/oirPztOntw2Gvlcw9xOlxDwDJI6bB87LLLpNNa5cA233lwnzV/6U+7DlnTD5Nz3mbBu3nGUyy9hDf6G3q35dTirzH/eqKEvblBIAEFNPORcB4du6ZJ1fuuYM8hzh50gR94fzTdfGsqfr3rQ1hr+N2GSovykm6aeSN3iYtrfSq1wxEvBm8GbDUawa0tNKrjd6mGFcIAIg2FhcBY+SWIZ3SNvOFf/gbTZ6UogdffEf7h+nDnow9xOtburWiqnbMPe7nzpjCtDsAJBBGPIEx+vFXiof0EL/0oVdUcv8m/euf9od9b7L2EL9rwyfT68FUfvdi3ful88Jegx73AJB4CJ7AGF1zwZl65PpSTXS7gk67B+N2GZrodumR60uT7lnFne+3q7q+NeLp9VAG9rgHACQGgicQBYtL87Vp2cKPV7oHf+5z4PGywmxtWrYw6UKnJD1T06gJYQL6wxUlunjWVH370iL5HvyifA9+UTOyJwU91+0yVLWjMValAgCijGc8gSjx5KZr/a0L+vej3O5r097mzv79KOfkZWq+Jzvp96Os9rXqRJjRzpW/26Wi3Ay9+0Gn1mzeI0k62t0T9FwzYGm7ry0mdQIAoo/gCURZcX7WoGBJB57B9jV3hX29s+eE/GZAH/lNHekKHjgH2tvcGa3SAAAxxlQ7EGOEzk8EApb8ZnT39vWblgJjfF4UAGAPgicA27hchlLc0Q3i9LgHgMRB8ARgq9l5GcOe03siEHGYTPYe9wCQSAieAGxV7skZdtup99s+VGnBFM3InqTstBQZIU5P5h73AJCICJ4AbFVRVjDsHp7/+qf9CgQsbV72Wf3l3kXKnxJ8O6Vk7XEPAImKVe0AbFWcn6XyohzVNLSFDKD1Ld3628f+HPY6bpeheYXZSb01FQAkGkY8Adhu1ZISuUPNn0coGXvcA0CiI3gCsJ0nN12rK0qG9LiPVLL2uAeARMdUO4C46GsXuqKqVqZlRdS73e0y5DYMra4oScp2owCQ6BjxBBA39LgHgOTCiCeAuKLHPQAkD4InAEegxz0AjH9MtQNwJEInAIw/BE8ACS0QwaIkAIAzMNUOIKH0PQta7WvVvuau/mdBZ+dlqNyTw7OgAOBgBE8ACcHX0q07N9Squr5VbpcxaPslv2lp96FO7TncpbVbG1RelKOfLj4vjtUCAIJhqh2A4230NmnRmi2qaWiTpJB7fvYdr2lo03WPvmFbfQCAyBA8ATjaRm+TllZ61WsGwm4yX/ndi3Xvl06OcpoBS72BgCTphZ2HbKkTADA8gicAx6pv6daKqlqNZfnQPz1bJ19Ld9RqAgCMHsETgGPdteFkO82xMGXpzg21UaoIADAWLC4C4Eg7329XdX1r0Ncmpbj1468U66rzz1B3zwk98af9Ia9jBixV17eqrqk94tXubF4PALFB8ATgSM/UNGqCy9CJIM91/uCac3VRUY5u+bcdOtrVqxVXna3zp0/WroMdQa/ldhmq2tEYMniyRRMA2IPgCcCRqn2tQUNn2kS3vjZ/hpY97dWf3zsqSfr++r9q2z2fC3ktM2Bpu69tyPGRbtG0akmJPLnpUfh2AJCceMYTgCPta+4KerxwappSJ7jlPXCs/1j7h37tbwl+fp+9zZ2Dfh7NFk2L1mzRRm9TpF8BAHAKRjwBOE4gYMlvRrcVpt+0+p/d7NuiaSSfYAYsmbK0tNIrSVpcmh/V+gAgGTDiCcBxXC5DKe7gi3sajh5X74mASmdO6T82edIEFQ0zBZ7iNuRyGWPeosmStKKqli2aAGAUCJ4AHGl2XkbQ48d7Ta3f0agfXHOuFpw1VZ86PUP/XDFXYfaWlyTNycuUFKUtmiy2aAKA0WCqHYAjlXtytOdwV9BnL3/ywm6lTXTrNzeWqbvnhP71T/XKPC0l5LXcLkPzPdlht2gaidFs0QQAYMQTgENVlBWEXPBzvNfU8vV/1Xn3vqT5D7ysJ7bs1/VPbNOPnt8V9HwzYKmirKB/i6aBrjgnT7X3LVLf4fPOnCzfg1/UXVed3X/Og0su0Jqvlw56X98WTQCAyBE8AThScX6Wyoty5B7jRu5ul6HyohwV52cF3aJpe32r0lMn6PzpJ0cuL5qVo6NdPbp41tT+cy4qmqpt+48Oel+oLZoAAKERPAE41qolJXIbYwyeMrRqSYmk4Fs0dfac0K6DHf1B8+JZU/Wb1+t13vTJSpvo1umTU1WUm643Twme0tAtmgAA4RE8ATiWJzddqytKNJbo+eOvFMuTmx52i6Y364/q4lk5kqT5nhy99PYHeq+5S/M9ObqoaKo+aP9IvqPHh7yvb4smAEBkWFwEwNH69stcUXVyNXqo5z4HcrsMTXQZkkxdc8GZkj7ZoilY+Ny2/6i+Vlag886crBNmQO8d6da2/a26eFaOsial6M36oaOd0idbNAEAIsOIJwDHW1yar03LFmpeYbYkDfvcZyBgaUb2JEnS7kOf9G8PtUVTte/kc543X1qkNz9e9b5t/1FdPGuqLpo19PnOPn1bNAEAIkPwBJAQPLnpWn/rAj3/vUv1dxfN1OxpGSGn4C1J+4+e3OC94vGt+trjW+Vr6Va5J/hipY4PT+idDzq0uHR6f8h8s75V50/P0lnTMvTm/qFbMPVt0QQAiBzBE0BCKc7P0qcLs3Wg9XjE09x9fdanZaaGnKp/c3+rJrhd/cGz/UO/9jV3qrnjI+0P0qWob4smAEDkeMYTQEIZS5/1f960R2dNS5fv6PEhAfRHz+8asg/oNf/yetDruV2G5hVms3k8AIwQI54AEkYkfdYfrijRE38/L+hrlqQDR49rrOuB3MYnWzQBACLHiCeAhBFJn/WVv9ulcFt/BiR5ctK0/0j3iEZN+xiSVleUyJObPop3A0ByY8QTQELo67M+3HZKnT0n1PHRiZCvmwFL7x3p1vcXfUoT3a6IOyO5XYYmul165PrS/i2eAAAjQ/AEkBCC9VkPJtxUex+3y9CRzp6ItmjqO15WmK1NyxYSOgFgDJhqB5AQgvVZH62+PusrFxdr/a0LVNfUrqodjdrua9Pe5k75TUspbkNz8jI135OtirICFhIBQBQQPAEkhGB91sdiYJ/14vysQcEyELDoSAQAMcBUOwDHC9dnfbTC9VkndAJAbBA8AYxaqOAWbX191qOJPusAYD+m2gFErO9ZyGpfq/Y1d/U/Czk7L0PlnpyYPgs5Oy9Duw91Dn9ihOizDgD2I3hi3OH5vOjztXTrzg21qq5vldtlDNrSyG9a2n2oU3sOd2nt1gaVF+Vo1ZLo73NZ7snRnsNdw26nFAn6rANAfBA8kfDiOQqXDDZ6m7Si6pON20MFv77jfX3RV1eURHXroYqyAq3d2hCVa9FnHQDig+CJhOWEUbjxbix90ZdWeiUpauHzvSORrWqf6Hapu9cM+Tp91gEgflhchIS00dukRWu2qKahTVLko3AbvU221SjZt/gmFiLpix6OJWlFVa18Ld1RqyUct+vkKPenC7O193DoZ0Hpsw4A8cOIJxKOk0bhTjWepv0j6Ys+HNOydOeGWq2/dUHMazn79ExtuO0Sbd1/VP/xZugpefqsA0D8EDyRUKI1Cjd3xpSoho/xNu3f1xd9rMyAper6VtU1tY86cEday65DHTr33j8oxW3Ib1pKdQc/76xpGaOqI1mxWA9ANBE8kVCcNArX54Wdh7Riw9txX3wTTX190YO1qHz9rsv129fr9ds3fP3HXviHS7Vp12E98se9Q853uwxV7WgcdfAMV4skVX73Yr37QafMgKXrLszXux906hv/ui3ouWOtJRmMp1F7AM5D8ETCcNIo3EB3bahVrxn5iJBd0/5jEYu+6LGsZcm8GfqPbQ366mN/jmkt49l4G7UH4EwETySM4Ua+Jrpduueac3Tt3OnKTJ2g2qZ2/e/nd6n2/fYh50Zj5Kvh6HFJcty0fzTEsi/6SEVSi6+lWw+++E7MaxmvnLJlFoDxj1XtSBjDjXzdc805urr4TP2v9X/VF3/xuhqOduvfvl2urEkpQ86NxsjXvb+rG9P7pU+m/Z3E7r7o0ahlZ9PQ/7iIdi3jVd9ivV4zEPHm/GbAUq8Z0NJKr+07RQBIbARPJIxwI1+TUtz6fy4q1E9e2K1X9xzRvuYu3b1hpz7yB/T1+cE3Ch/LyNfO99v7t3Iai4HT/k4xXF/0QEAyjMGvT3CH/7+S0fZFj7RH+4dh9u2MVi3jkZO2zAKQHAieSAjDjXwVTk3TxAmuQWHwRMDSX98/ptl5wVcxj2Xkq2/aP5j0iW498vVS7frRF1T9g8/p5kuLVPndi3Xvl84Len7ftL+ThPozk6TW7h5Ny0zt/zkjdYIKstPCXm8sfdHD1TIa9Gj/RDQX6wFAJAieSAiRjnyNxFhGvsJN+//Tl85TmSdb31m7Q3/3mzc135Oj86dPDnktJy54KffkyB3iz+bP7x3V316Yr/mebJ19eqb++Wtzw4aXsfZFD1fLSNGj/RN9i/UinV4PxYmj9gCci+CJhBFu5Kvh6HH1nDA1r/CTUDHBZahkRpb2Hg4+RT+Wka9Q0/7pE91a8ukZeuD3u/Xn945qz+Euraj667DByWkLXirKCkIGkv/z6nt6s75Vv7lpvn77rfna9PYHOnA09FTrWPuih6tlpOjR/olgo/aV371Y93/5fN37pfP01/sWafs/fl7Xzy/QpBS3Vn+1RHUrv6BX/9dluuxT0wa9z4mj9gCciVXtSBjlnhztOdwVNIR86Df1n9sO6AfXnKv2D/1qOvah/t/PztKkFLee3nFgyPljGfnqm/YPtkH5zI+n/P/aeKz/WGfPCe0/Ev4ZuL5pf6c8e1icn6XyohzVNLQN+fPu6jmh7/1/fxl0bMNbwReYRKMverhaJOn6J4Lv2RmLWsaTUKP2Sz6dr8e37NfiX76uL82drh9fV6wvnH+GXnr7Az36yj7dfOks/ezrpbrkwZf1kT8gyZmj9gCciRFPJIzhRr4e+sM7erHukH72tbn6/fcuVeHUdH3zt9Xq+PDEkHPHMvLltGl/KTY94VctKZHbGNv3jFZfdCfVMl6EGrXffahTv/zvffIdPa7/88o+9ZwIqPV4ryq3N8p39Lj+5eW9ykmfqHPPGPz4iNNG7QE4EyOeSBjDjXz1nAho5X/t0sr/2hX2OtEY+Zqdl6H9zR1Djh84ely9JwIqKZiig+0fSJIyUyeoKDc97Ob3I532t6O7jCc3XasrSrS00juqVc+GotcX3Um1jAfhFuu988En/14HLKnteK/e/eCTUHmkq0eSNDVj4qD3OW3UHoAzETyRUFYtKdGiNVtkjnoDmOiMfJV7ctTQMnSEp7vX1Ia33tcPrj5X7cf9aunq0bIrP6WAZckKUfNIpv3t7i7Ttzl43+bikTxr6XYZchtG1DcXH00tkjTR5dKqirlsdD5A36h9sPB5IuixwNBrnDICzTZVACLBVDsSSt/I12h/vUVr5CvctP+Pn9+ltw606Tc3lek/v3ORahra9F5zl3r8Q395S5FP+2/0NmnRmi39W0ZF2l1mrBt8Ly7N16ZlC/sXboVaKNV3vKwwW5uWLYxJ0BtpLZL03O2fIXQGwTZVAOKBEU8kHCeMwhXnZ30cflqGvNbda2rp097+nyeluPU/PzdH66qHrvqNdNq/r7vMSMZ5o9kT3pObrvW3Luif4t/ua9Pe5s7+Kf45eZma78mOyhR/tGpZcuGZqv/L65o5Nfweo8kq3GK9kWKbKgCRIngiIS0uzdfcGVNCTjv36TteVpith8Y47Xyq//3lYtW9+eqQ4+dPn6yzpmXI23hMmadN0P/83BxJ0uZdHwytL4Jp/2h1l4lGT/ji/KxBwTKez/QNV4vf71f9X4K9E9LJUfu1Wxuici22qQIQKYInEla8R+FmTk1TnRR02v+Wv5mlWdPS5TcD2tnUropfbVXbcf+gcyKd9o9md5n1ty4Y03VO5aRn+pxUSyIItlgv2NZUlz70ypBjnrt/3//PbFMFYCQInkh48R6Fe2hJiVZseLt/2v/tgx269pevhzx/JNP+fd1lxmpgdxkCAvo4ZbEegOTB4iKMO3aPfF1zwZkxW3wTqid8TvpEbf/Hz+l/XHZW/7FPz8zWnh9frUvOmhry8+kug4GcslgPQPJgxBOIglhN+4fqLtPa3asVz9Tqib8v05/2tmj/kS6t+fpc/dtWn/783tGg16K7DIJxwmI9AMmD4AlEUbSn/UN1l5GkV989osrtB/TI9aXa+X67jveaWvWHd8Nej+4yCMYJi/UAJAeCJxBDY22DGaq7TJ8Hfr9bm5Yt1DUXnKlrf/G6eoNs9D0Q3WUQSrwX6wFIDgRPwKHCdZfpUzg1TadPPk0uQ5qRM0nvHg4/okl3GQwn3ov1AIxvLC4CHCxcd5kUt6FHvl6q52sP6meb9+jBvy3R1PSJIc+X6C6DkSN0AogmgifgYOWenJCr5P/XorOVeVqK7v/dLj322nuqb+nWqq+G3taG7jIAgHgjeAIOFqon/MWzcvTtS4u07GmvunpOyLKk5eu9ml+Uo7+7aGbQa9FdJroCUWg1CQDJhmc8AQcL1l1Gkrbtb9Wcf3xx0Lnvt32okvs3Bb0O3WXGrm/RTbWvVfuau/oX3czOy1C5J4dFNwAQAYIn4HB0l4kvX0t3yG2G/Kal3Yc6tedwl9ZubVB5UY5Wsc0QAITEVDvgcHSXiZ+N3iYtWrNFNQ0nN94Ptbl63/GahjYtWrNFG71NttUIAImE4AkkgMWl+Xrk+lJNdLtCLjY6ldtlaKLbpUeuL6W7zChs9DZpaaVXvWZgUOCs/O7FuvdL5wV9jxmw1GsGtLTSS/gEgCAInkCCWFyaH7Oe8BisvqVbK6pqR/1wg6WTLSh9Ld3RLAsAEh7PeAIJhO4y9rhrw8m+5WNhWpbu3FCr9bcuiFJVAJD4CJ5AAqK7TOzsfL9d1fWtEZ9/+dl5+vk3SvXD5+q00Xuw/7gZsFRd36q6pnb+IwAAPsZUOzAOEDqj55maRk2I8M/zy3On61++UfrxM50Hh7zudhmq2tEY7RIBIGEx4gkAA1T7WnUigs3h//7iQq34wtn6ztodejPECKkZsLTd1xbtEgEgYRE8AWCAfc1dw55z9QVnaGp6qr76qz+r9v32sOfube6MVmnjEo+JAMmF4Imkwi85hBMIWPKbw492vn2wQ8XTs/S1soJhg6fftPj3boC6pnZt2NGgUkkX/miTuvyiAxSQRAieGNdoc4iRcLkMpbiNYcPngaPH9cDvd6vyuxfLDFi673dvhzw3xW0QOjW4A1RailRaJvkDliSDDlBAEiF4YlyizSFGa3ZehnYfGn56vL6lW994Ylt/+PzR87uCnjcnLzPaJSacjd4mraj6ZIuqSDtAra4oYR9aYJxhVTvGHdocYizKPTkRd4fa39Ktb/zrm7p27nT94xfPHfK622Vovic72iUmlFAdoMKhAxQwfjHiiXGl75fcSLb+NgOWTFlaWumVJEZYklxFWYHWbm0I+fr1T2wb9PN7R7o0/4E/Bj3XDFiqKCuIan2JJFodoObOmMKMBDBOMOKJcYM2h4iG4vwslRdFPuoZittlqLwoJ6mfIY5mBygA4wPBE+MGv+QQLauWlMhtjDF4GoZWLSmJUkWJp68DVKTT66EM7AAFIPERPDEu8EsO0eTJTdfqihKNNnoaklZXJPeCtXAdoP7t5gW64IIL9MNri1V7/yK99cMrtfzKT4W8Fh2ggPGDZzwxLvT9kgvWcWZG9iS9ftcVQ45v2390yPN60ie/5JJ5ihSfPOvbtxo7kv+ocbsMuQ2D1dgavgPUzJkz9adDB3TdL9/QBTOy9NO/vUAHj32oyu1DAyYdoIDxg+CJcSHcL7mDxz7U/B9/svhjWmaq/uM7F9HmEMNaXJqvuTOmhNyaq0/f8bLCbD3E1lyShu8A9eGHH+onL+xSj2lof0u3zjkjUzdfWhQ0eEp0gALGC4InxoVwv+QClnSkq0eSlDrBpSe+OU9vHWjTI3/cE/I9/JJDH09uutbfuqC/GcF2X5v2Nnf2NyOYk5ep+Z5smhEMEEkHqNbWwf/h99aBY/rO38ySyzj5d/ZUdIACxgeCJxJepG0OJWnVV0uUnjpBf/frNxVuHRK/5HCq4vysQcGSfz9Ci7QD1EjQAQoYH1hchITX90tuOHdcMVsL50zTd9buUHevGfZcfslhOPz7Ed7svIywr2dnD95Y/8KCKfK1dAcd7ZToAAWMF7YEz0cffVQej0ennXaaLrroIlVXV9vxsUgiw/2Su6r4DP3DFXN0+7q3dKD1+LDX45ccMDbDdYBKS0vT3Vefp1m56fry3Om68RKPnnzDF/RcOkAB40fMg+fTTz+t5cuX67777tNbb72luXPn6gtf+IKam5tj/dFIIuF+yX3q9Az97Gtz9avX3tPew12alpGqaRmpypqUEvR8fskBY1dRVhB2J4DGxkadluLSc3d8Rj9afL6efMOnddUHgp6b7B2ggPEk5s94/uxnP9Mtt9yib33rW5KkX/3qV/r973+v3/72t7r77rsHndvT06Oenp7+nzs6OiRJfr9ffr8/1qU6Tt93TsbvPlJLLjxTldU+TXAPfe3TBVlKmzhB//C5OfqHz83pP/7m/qP65m+2BrmapSUXnjnsnzv3x9m4P/F1dl6aLpk1Rd7GY0MCqMuwFAgE9NPnd+n+39X1H08N8vfX7TJUWjBFZ+elcS9twt8d53LqvRlJPYZljbHVSxi9vb1KS0vTM888o+uuu67/+I033qhjx45p48aNg86///77tXLlyiHXWbdundLS0mJVJgDARp/5zGfU3t6uurq64U8G4HjHjx/XDTfcoPb2dk2ePDnsuTEd8WxpaZFpmjr99NMHHT/99NP1zjvvDDn/nnvu0fLly/t/7ujoUEFBgRYtWjTsFxmP/H6/Nm/erCuvvFIpKcGnhfGJA0eP67pH31BvIDDqa0x0ufTc7Z/RzKnD/4cO98fZuD/O8MLOQ7prQ60GjnD8R4mUI+mHO1zqCYR+DtSQ9NCSEl1zwZmxLhMD8HfHuZx6b/pmqCPhqO2UUlNTlZqaOuR4SkqKo/6A7Zbs3z9SZ52RpR8vmaullV6NZhjfkLSqYq7OOmNkezFyf5yN+xNfiz89U3K5B3WA+rtfb9OqclM9Abd6zKHBs68D1Co6QMUVf3ecy2n3ZiS1xHRxUW5urtxutw4fPjzo+OHDh3XGGWfE8qORpBaX5uuR60s10e0Ku6J2ILfL0ES3S49cX8ovOSAGFpfma9OyhZpXeHLRXqi/m33HywqztWnZQv4+AuNQTEc8J06cqHnz5unll1/uf8YzEAjo5Zdf1h133BHLj0YSo80h4DwDO0Bt2NEgqV4pLkM9pugABSSRmE+1L1++XDfeeKPKyspUXl6uRx55RN3d3f2r3IFYoM0h4EzF+Vk6O+9cvfBCvf5y7yK53RPYjB9IIjEPnl//+td15MgR3Xvvvfrggw9UWlqqP/zhD0MWHAGxQJtDwNn4+wgkF1sWF91xxx1MrcMR+CUHAED80KsdAAAAtiB4AgAAwBYETwAAANiC4AkAAABbEDwBAIiiQJB9gwGc5KiWmQAAJJq+/YKrfa3a19zVv1/w7LwMlXty2C8YGIDgCQDAKPhaukN2SPOblnYf6tSew11au7VB5UU5WkWHNICpdgAARmqjt0mL1mxRTUObJAVtyzvweE1Dmxat2aKN3ibbagSciBFPAABGYKO3SUsrvRrJk5xmwJIpS0srvZKkxaX5MakNcDpGPAEAiFB9S7dWVNWOKHQOZElaUVUrX0t3NMsCEgbBEwCACN21oVamNbZV66Zl6c4NtVGqCEgsBE8AACKw8/12Vde3hnyeM1JmwFJ1favqmtqjVBmQOHjGEwCACDxT06gJLkMnQgTPSSlu/fgrxbrq/DPU3XNCT/xpvz5/7unadbBDP3p+16Bz3S5DVTsa2WYJSYfgCQBABKp9rSFDpyT94JpzdVFRjm75tx062tWrFVedrfOnT9augx1DzjUDlrb72mJZLuBITLUDABCBfc1dIV9Lm+jW1+bP0E9e2K0/v3dU7x7u1PfX/1UTXKF/ze5t7oxFmYCjETwBABhGIGDJb4Ye7SycmqbUCW55DxzrP9b+oV/7W0KHVb9p0V4TSYfgCQDAMFwuQyluI6rXTHEbcrmie03A6QieAABEYHZeRsjXGo4eV++JgEpnTuk/NnnSBBWFaZE5Jy8zmuUBCYHFRQAARKDck6M9h7uCbqd0vNfU+h2N+sE156rtuF9Hu3q04gtnK9RMuttlaL4nO8YVA85D8AQAIAIVZQVau7Uh5Os/eWG30ia69Zsby9Tdc0L/+qd6ZZ6WEvRcM2CpoqwgVqUCjkXwBAAgAsX5WSovylFNQ1vIUc/l6/+q5ev/2n/sinPyhpzndhmaV5jNHp5ISjzjCQBAhFYtKZHbGNuCILdhaNWSkihVBCQWgicAABHy5KZrdUWJRhs9DUmrK0rkCbPoCBjPmGoHAGAEFpfmS5JWVNXKtKywvduvf2KbpJPT627D0OqKkv73A8mIEU8AAEZocWm+Ni1bqHmFJ1emu0Psx9l3vKwwW5uWLSR0Iukx4gkAwCh4ctO1/tYFqmtqV9WORm33tWlvc6f8pqUUt6E5eZma78lWRVkBC4mAjxE8AQAYg+L8rEHBMhCw6EgEhMBUOwAAUUToBEIjeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGxB8AQAAIAtCJ4AAACwBcETAAAAtiB4AgAAwBYETwAAANiC4AkAAABbEDwBAABgC4InAAAAbEHwBAAAgC0IngAAALAFwROAYwUCVrxLAABE0YR4FwAAfeqa2lW1o1HVvlbta+6S37SU4jY0Oy9D5Z4cVZQVqDg/K95lAgBGieAJIO58Ld26c0Otqutb5XYZMgeMdPpNS7sPdWrP4S6t3dqg8qIcrVpSIk9uehwrBgCMBlPtAOJqo7dJi9ZsUU1DmyQNCp0D9R2vaWjTojVbtNHbZFuNAIDoYMQTQNxs9DZpaaVXI3mS0wxYMmVpaaVXkrS4ND8mtQEAoo8RTwBxUd/SrRVVtYNCZ+V3L9a9XzovovdbklZU1crX0h2T+gAA0UfwBBAXd22olWmNbdW6aVm6c0NtlCoCAMQawROA7Xa+367q+taQz3NGygxYqq5vVV1Te5QqAwDEEs94ArDdMzWNmuAydCJI8HS7DK388vn6yqfzdcK09B/bGvSzzXtCXsvtMlS1o5FtlgAgATDiCcB21b7WoKFTkpbMmyEzYOm6X76hlf/1tr7zN0W6fn5ByGuZAUvbfW2xKhUAEEWMeAKw3b7mrpCvHTr2oX70/C5J0v6Wbp1zRqZuvrRIldsbQ75nb3Nn1GsEAEQfI54AbBUIWPKboZ/t/EvjsUE/v3XgmDy56XIZoa/pNy3aawJAAiB4ArCVy2UoxR0mRY5CituQK1wyBQA4AsETgO1m52WEfK20YMqgny8smCJfS7fCDWjOycuMUmUAgFgieAKwXbknR+4QI5TTp0zSP33xXM3KTdeX507XjZd49OQbvpDXcrsMzfdkx6hSAEA0sbgIgO0qygq0dmtD0Nf+71vv67QUt5674zMKBCw9+YZP66oPhLyWGbBUURZ61TsAwDkIngBsV5yfpfKiHNU0tA3aRP76J7b1//M/PVc37HXcLkPzCrPZwxMAEgRT7QDiYtWSErmNsS0IchuGVi0piVJFAIBYI3gCiAtPbrpWV5RotNHTkLS6okSe3PRolgUAiCGm2gHEzeLSfEnSiqpamZYVUe92t8uQ2zC0uqKk//0AgMTAiCeAuFpcmq9NyxZqXuHJlemhVrv3HS8rzNamZQsJnQCQgBjxBBB3ntx0rb91geqa2lW1o1HbfW3a29wpv2kpxW1oTl6m5nuyVVFWwEIiAEhgBE8AjlGcnzUoWAYCFh2JAGAcYaodgGMROgFgfCF4AgAAwBYETwAAANiC4AkAAABbEDwBAABgC4InAAAAbEHwBAAAgC0IngAAALAFwRMAAGCcCQSseJcQFJ2LAAAAElxfy+FqX6v2NXf1txyenZehck+OY1oOEzwBAAASlK+lW3duqFV1favcLkPmgJFOv2lp96FO7TncpbVbG1RelKNVS0rkyU2PW71MtQMAACSgjd4mLVqzRTUNbZI0KHQO1He8pqFNi9Zs0UZvk201nooRTwAAgASz0dukpZVejeRJTjNgyZSlpZVeSdLi0vyY1BYOI54AAAAJpOHoca2oqh1R6BzIkrSiqla+lu5olhURgicAAEACufd3dTKtsa1aNy1Ld26ojVJFkSN4AgAAJJCahraQz3NGygxYqq5vVV1Te5SqigzBEwAAIIFMcBlRuY7bZahqR2NUrhUpgicAAEACORGlzeHNgKXtvraoXCtSBE8AAIAktbe509bPI3gCAAAkgFi0wfSblq3tNQmeAAAACcAVpWc7B0pxGzG5bigETwAAgHHkmwsK9Z/fuSiic+fkZca4msEIngAAAAnEPcwIZU76RBVOTYvoOvM92dEqKyK0zAQAAEggJ/fwDB0+H/njXj3yx70RXaeirCCKlQ2PEU8AAIAEMq8we9hRz+G4XYbKi3JUnJ8VpaoiQ/AEAABIIP/7y8VyG2MMnoahVUtKolRR5AieAAAACWTm1DStrigJM9keniFpdUWJPLnp0SwrIjzjCQAAkGAWl+ZLklZU1cq0rIh6t7tdhtyGodUVJf3vtxsjngAAAAlocWm+Ni1bqHmFJ1emh3rus+94WWG2Ni1bGLfQKTHiCQAAkLA8uelaf+sC1TW1q2pHo7b72rS3uVN+01KK29CcvEzN92SroqzA9oVEwRA8AQAAElxxftagYBkIWLZ2JIoUU+0AAADjjBNDp0TwBAAAgE0IngAAALAFwRMAAAC2IHgCAADAFgRPIIkFIthwGACAaGE7JSCJ9O3zVu1r1b7mrv593mbnZajck+OYfd4AAOMTwRNIAr6Wbt25oVbV9a1yu4xBrdX8pqXdhzq153CX1m5tUHlRjlYtiU8PXwDA+MZUOzDObfQ2adGaLappaJOkkP18+47XNLRp0Zot2uhtsq1GAEByiFnwfOCBB3TJJZcoLS1NU6ZMidXHAAhjo7dJSyu96jUDIQPnqcyApV4zoKWVXsInACCqYhY8e3t7VVFRodtuuy1WHwEgjPqWbq2oqtVolw9ZklZU1crX0h3NsgAASSxmwXPlypVatmyZLrjgglh9BIAw7tpQK9Ma26p107J054baKFUEAEh2jlpc1NPTo56env6fOzo6JEl+v19+vz9eZcVN33dOxu+eCJx8f3Yd7NBfDxzVBEOa4B76umFIN196lr42f6bOzDpNLV29enp7g3716r5TzrT01wNHVXvgqM49c7IttUeLk+9PsuPeOBv3x7mcem9GUo9hWWMcEhnGU089paVLl+rYsWPDnnv//fdr5cqVQ46vW7dOaWlpMagOSE7nnXeeCgsLVVdXp6NHj+q0005TRkaGDhw4EO/SAAAJ5vjx47rhhhvU3t6uyZPDD1KMaMTz7rvv1kMPPRT2nN27d+ucc84ZyWX73XPPPVq+fHn/zx0dHSooKNCiRYuG/SLjkd/v1+bNm3XllVcqJSUl3uXgFE6+P0se+7PePdwZ9LX0iW5tvWaWVj5fp2d2HPz4aI+kdklBhkclnXP6ZD1z24KY1BorTr4/yY5742zcH+dy6r3pm6GOxIiC5/e//33ddNNNYc+ZNWvWSC45SGpqqlJTU4ccT0lJcdQfsN2S/fs7nRPvz+7D3fKbRtDXzpmaqdQUt17bc1Q9Ic451a7DXY77jpFy4v3BSdwbZ+P+OJfT7s1IahlR8Jw2bZqmTZs24oIA2CcQsOQ3Qz9B85E/MOJr+k1LgYAllyuyoAoAQDAxW9V+4MABeb1eHThwQKZpyuv1yuv1qqurK1YfCUCSy2UoxR06IPqOduvDXlOfmZ0b8TVT3AahEwAwZjFb1X7vvfdq7dq1/T9feOGFkqRXXnlFl112Waw+FoCk2XkZ2n0o+DOePScC+tVr7+meq8+R3wxoh69NU9Mnas7pmVq/ozHoe+bkZcayXABAkohZ8Hzqqaf01FNPxeryAMIo9+Roz+GukN2K/uW/9+pEwNLyKz+lvMzT1Nz5kda9GXxFu9tlaL4nO5blAgCShKP28QQQHRVlBVq7tSHk65YlPfrKPj36yqn7dg5lBixVlBVEszwAQJKK2TOeAOKnOD9L5UU5co/xuUy3y1B5UY6K87OiVBkAIJkRPIFxatWSErmNMQZPw9CqJSVRqggAkOwInsA45clN1+qKEo02ehqSVleUyJObHs2yAABJjGc8gXFscWm+JGlFVa1Mywq52Gggt8uQ2zC0uqKk//0AAEQDI57AOLe4NF+bli3UvMKTK9NDPffZd7ysMFubli0kdAIAoo4RTyAJeHLTtf7WBapralfVjkZt97Vpb3On/KalFLehOXmZmu/JVkVZAQuJAAAxQ/AEkkhxftagYEkbTACAnZhqB5IYoRMAYCeCJwAAAGxB8AQAAIAtCJ4AAACwBcETAAAAtiB4AgAAwBYETwAAANiC4AkAAABbEDwBjEoggr7vAAAMROciABHpa7dZ7WvVvuau/nabs/MyVO7Jod0mAGBYBE8AYflaunXnhlpV17fK7TJkDhjp9JuWdh/q1J7DXVq7tUHlRTlataREntz0OFYMAHAqptoBhLTR26RFa7aopqFNkgaFzoH6jtc0tGnRmi3a6G2yrUYAQOJgxBNAUBu9TVpa6dVInuQ0A5ZMWVpa6ZUkLS7Nj0ltAIDExIgngCHqW7q1oqp2RKFzIEvSiqpa+Vq6o1kWACDBETwBDHHXhlqZ1thWrZuWpTs31EapIgDAeEDwBDDIzvfbVV3fGvJ5zkiZAUvV9a2qa2qPUmUAgETHM54ABnmmplETXIZOBAmeld+9WLsPdajnREDXzy+Q3wzoP988oEf+uDfotdwuQ1U7GtlmCQAgiRFPAKeo9rUGDZ19lsyboQ97TV336Bv66Yvv6B+umKNLZ+cGPdcMWNrua4tVqQCABEPwBDDIvuausK+/c6hTP395r3xHj+v/vtWk2qZ2fWb21JDn723ujHaJAIAERfAE0C8QsOQ3wz/b+c4HHYN+PtL5kaZmpIY8329atNcEAEgieAIYwOUylOI2wp5z4pRgalmSK8xbUtyGXOFOAAAkDYIngEFm52VE9Xpz8jKjej0AQOIieAIYpNyTI3eURijdLkPzPdlRuRYAIPERPAEMUlFWMOY9PPuYAUsVZQVRuRYAIPGxjyeAQYrzs1RelKOahrYhAfT6J7YNOf+7/14T9Dpul6F5hdns4QkA6MeIJ4AhVi0pkdsY23S72zC0aklJlCoCAIwHBE8AQ3hy07W6okSjjZ6GpNUVJfLkpkezLABAgmOqHUBQi0vzJUkrqmplWlZEz326XYbchqHVFSX97wcAoA8jngBCWlyar03LFmpe4cmV6aFWu/cdLyvM1qZlCwmdAICgGPEEEJYnN13rb12guqZ2Ve1o1HZfm/Y2d8pvWkpxG5qTl6n5nmxVlBWwkAgAEBbBE0BEivOzBgXLQMCiIxEAYESYagcwKoROAMBIETwBAABgC4InAAAAbEHwBAAAgC0IngAAALAFwRMAAAC2IHgCAADAFgRPAAAA2ILgCQAAAFsQPAEAAGALgicAAABsQfAEAACALQieAAAAsAXBEwAAALYgeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGxB8AQAAIAtCJ4AAACwBcETAAAAtiB4AgAAwBYETwAAANiC4AkAAABbEDwBAABgC4InAAAAbEHwBAAAgC0IngAAALAFwRMAAAC2IHgCAADAFgRPAAAA2ILgCQAAAFsQPAEAAGALgicAAABsQfAEAACALQieAAAAsAXBEwAAALYgeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGxB8AQAAIAtCJ4AAACwBcETAAAAtiB4AgAAwBYETwAAANiC4AkAAABbEDwBAABgC4InAAAAbEHwBAAAgC0IngAUCFjxLgEAkAQmxLsAAPara2pX1Y5GVftata+5S37TUorb0Oy8DJV7clRRVqDi/Kx4lwkAGGcInkAS8bV0684Ntaqub5XbZcgcMNLpNy3tPtSpPYe7tHZrg8qLcrRqSYk8uelxrBgAMJ4w1Q4kiY3eJi1as0U1DW2SNCh0DtR3vKahTYvWbNFGb5NtNQIAxjdGPIEksNHbpKWVXo3kSU4zYMmUpaWVXknS4tL8mNQGAEgejHgC41x9S7dWVNWOKHQOZElaUVUrX0t3NMsCACQhgicwzt21oVamNbZV66Zl6c4NtVGqCACQrAiewDi28/12Vde3hnyeM1JmwFJ1favqmtqjVBkAIBkRPIFx7JmaRk1wGUFf+0Z5gd78wedknPLyv35znlZ9tWTI+W6XoaodjbEoEwCQJAiewDhW7WvViRCjnb/feUhT0lK0YNbU/mNZk1K08FPT9Nxfhq5kNwOWtvvaYlYrAGD8I3gC49i+5q6Qr3V8eEKvvXtk0Gr1ay44Q23dfm3dfzToe/Y2d0a9RgBA8iB4AuNUIGDJb4Z/tvM5b5OuLj5DE90n/6/gutJ8/VftQYVai+Q3LdprAgBGjeAJjFMul6EUd/DnO/u8vLtZMqTLz8nTmVmnab4nJ+g0e58UtyFXiGdGAQAYDhvIA+PY7LwM7T4Uenq850RAL9V9oOsunC7P1DTtb+nW2wc7Qp4/Jy8zFmUCAJIEI57AOFbuyZF7mBHK57xNuuLsPH2trEDPhWmP6XYZmu/JjnaJAIAkErPg6fP5dPPNN6uoqEiTJk3SWWedpfvuu0+9vb2x+kgAp6goKxh2D88/v3dUxz7066y8jLB92c2ApYqygmiXCABIIjGban/nnXcUCAT0+OOPa/bs2aqrq9Mtt9yi7u5uPfzww7H6WAADFOdnqbwoRzUNbSEDqGVJF/3k5bDXcbsMzSvMVnF+VizKBAAkiZgFz6uuukpXXXVV/8+zZs3Su+++q8cee4zgCdho1ZISLVqzReaou7VLbsPQqiVDN5UHAGAkbF1c1N7erpycnJCv9/T0qKenp//njo6Tixz8fr/8fn/M63Oavu+cjN89ESTK/cnPmqjVS87XXRtqRxU9DUkPLTlf+VkTHf9dB0qU+5OMuDfOxv1xLqfem5HUY1hWqB37omvfvn2aN2+eHn74Yd1yyy1Bz7n//vu1cuXKIcfXrVuntLS0WJcIAACAETp+/LhuuOEGtbe3a/LkyWHPHXHwvPvuu/XQQw+FPWf37t0655xz+n9uamrSZz/7WV122WX69a9/HfJ9wUY8CwoK1NLSMuwXGY/8fr82b96sK6+8UikpKfEuB6dIxPtz4Ohx/fB3dappaJPbZQR97rPveFlhtn705WLNnJqY/9GXiPcnWXBvnI3741xOvTcdHR3Kzc2NKHiOeKr9+9//vm666aaw58yaNav/nw8ePKjLL79cl1xyiZ544omw70tNTVVqauqQ4ykpKY76A7Zbsn9/p0uk+3PWGVla993PqK6pXVU7GrXd16a9zZ3ym5ZS3Ibm5GVqvidbFWUF42YhUSLdn2TDvXE27o9zOe3ejKSWEQfPadOmadq0aRGd29TUpMsvv1zz5s3Tk08+KZeLbUMBJyjOzxoULAMBi45EAICYi9nioqamJl122WUqLCzUww8/rCNHjvS/dsYZZ8TqYwGMAqETAGCHmAXPzZs3a9++fdq3b59mzJgx6DWb1jMBAADAQWI2933TTTfJsqyg/wMAAEDy4aFLAAAA2ILgCQAAAFsQPAEAAGALgicAAABsQfAEAACALQieAAAAsAXBEwAAALYgeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGwxId4FhGNZliSpo6MjzpXEh9/v1/Hjx9XR0aGUlJR4l4NTcH+cjfvjXNwbZ+P+OJdT701fTuvLbeE4Onh2dnZKkgoKCuJcCQAAAMLp7OxUVlZW2HMMK5J4GieBQEAHDx5UZmamDMOIdzm26+joUEFBgRobGzV58uR4l4NTcH+cjfvjXNwbZ+P+OJdT741lWers7NT06dPlcoV/itPRI54ul0szZsyIdxlxN3nyZEf9C4bBuD/Oxv1xLu6Ns3F/nMuJ92a4kc4+LC4CAACALQieAAAAsAXB08FSU1N13333KTU1Nd6lIAjuj7Nxf5yLe+Ns3B/nGg/3xtGLiwAAADB+MOIJAAAAWxA8AQAAYAuCJwAAAGxB8AQAAIAtCJ4AAACwBcEzQfh8Pt18880qKirSpEmTdNZZZ+m+++5Tb29vvEuDpAceeECXXHKJ0tLSNGXKlHiXk/QeffRReTwenXbaabroootUXV0d75IgacuWLbr22ms1ffp0GYah5557Lt4l4WM//elPNX/+fGVmZiovL0/XXXed3n333XiXhY899thjKikp6e9YtGDBAr344ovxLmtUCJ4J4p133lEgENDjjz+ut99+W2vWrNGvfvUr/eAHP4h3aZDU29uriooK3XbbbfEuJek9/fTTWr58ue677z699dZbmjt3rr7whS+oubk53qUlve7ubs2dO1ePPvpovEvBKV577TXdfvvt2rZtmzZv3iy/369Fixapu7s73qVB0owZM/Tggw+qpqZGO3bs0BVXXKHFixfr7bffjndpI8Y+ngls9erVeuyxx7R///54l4KPPfXUU1q6dKmOHTsW71KS1kUXXaT58+frl7/8pSQpEAiooKBA3/ve93T33XfHuTr0MQxDzz77rK677rp4l4Igjhw5ory8PL322mtauHBhvMtBEDk5OVq9erVuvvnmeJcyIox4JrD29nbl5OTEuwzAMXp7e1VTU6PPf/7z/cdcLpc+//nPa+vWrXGsDEgs7e3tksTvGAcyTVOVlZXq7u7WggUL4l3OiE2IdwEYnX379ukXv/iFHn744XiXAjhGS0uLTNPU6aefPuj46aefrnfeeSdOVQGJJRAIaOnSpfrMZz6j4uLieJeDj+3cuVMLFizQRx99pIyMDD377LM677zz4l3WiDHiGWd33323DMMI+79Tf2E2NTXpqquuUkVFhW655ZY4VT7+jebeAECiu/3221VXV6fKysp4l4IBzj77bHm9Xr355pu67bbbdOONN2rXrl3xLmvEGPGMs+9///u66aabwp4za9as/n8+ePCgLr/8cl1yySV64oknYlxdchvpvUH85ebmyu126/Dhw4OOHz58WGeccUacqgISxx133KHnn39eW7Zs0YwZM+JdDgaYOHGiZs+eLUmaN2+etm/frp///Od6/PHH41zZyBA842zatGmaNm1aROc2NTXp8ssv17x58/Tkk0/K5WLAOpZGcm/gDBMnTtS8efP08ssv9y9aCQQCevnll3XHHXfEtzjAwSzL0ve+9z09++yzevXVV1VUVBTvkjCMQCCgnp6eeJcxYgTPBNHU1KTLLrtMhYWFevjhh3XkyJH+1xjJib8DBw6otbVVBw4ckGma8nq9kqTZs2crIyMjvsUlmeXLl+vGG29UWVmZysvL9cgjj6i7u1vf+ta34l1a0uvq6tK+ffv6f66vr5fX61VOTo5mzpwZx8pw++23a926ddq4caMyMzP1wQcfSJKysrI0adKkOFeHe+65R1dffbVmzpypzs5OrVu3Tq+++qpeeumleJc2chYSwpNPPmlJCvo/xN+NN94Y9N688sor8S4tKf3iF7+wZs6caU2cONEqLy+3tm3bFu+SYFnWK6+8EvTvyY033hjv0pJeqN8vTz75ZLxLg2VZ3/72t63CwkJr4sSJ1rRp06zPfe5z1qZNm+Jd1qiwjycAAABswUOCAAAAsAXBEwAAALYgeAIAAMAWBE8AAADYguAJAAAAWxA8AQAAYAuCJwAAAGxB8AQAAIAtCJ4AAACwBcETAAAAtiB4AgAAwBb/P2EdSIOIH2upAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(num_words):\n",
    "\t# sample from the model\n",
    "\tg = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "\tfor _ in range(num_words):\n",
    "\t\t\n",
    "\t\tout = []\n",
    "\t\tcontext = [0] * block_size # initialize with all ...\n",
    "\t\twhile True:\n",
    "\t\t\temb = C[torch.tensor([context])] # (1,block_size, embedding_size)\n",
    "\t\t\th = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "\t\t\tlogits = h @ W2 + b2\n",
    "\t\t\tprobs = F.softmax(logits, dim=1) # exponentiates, normalises (sum to one). It's like entropy, but goes from logits -> prob distr, instead of logits -> loss\n",
    "\t\t\tix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "\t\t\tcontext = context[1:] + [ix]\n",
    "\t\t\tout.append(ix)\n",
    "\t\t\tif ix == 0:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\tprint(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "quille.\n",
      "mikimli.\n",
      "jehty.\n",
      "halani.\n",
      "ejrahiel.\n",
      "meliah.\n",
      "jireei.\n",
      "jermara.\n",
      "chaiir.\n",
      "jaleigh.\n",
      "hamon.\n",
      "cadesinn.\n",
      "sroilet.\n",
      "jamii.\n",
      "wazero.\n",
      "diaryni.\n",
      "jimei.\n",
      "irsa.\n",
      "mem.\n"
     ]
    }
   ],
   "source": [
    "sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.08,  0.12,  0.94,  0.48],\n",
       "         [-1.51, -1.58, -0.60, -1.02],\n",
       "         [ 0.58,  0.58,  0.70, -0.77],\n",
       "         [-0.01, -1.51, -3.05, -1.97]],\n",
       "\n",
       "        [[-1.29,  0.11,  0.03, -1.84],\n",
       "         [-0.15, -1.35,  0.28, -0.60],\n",
       "         [ 1.28,  0.78, -0.53, -1.67],\n",
       "         [-0.28,  0.66, -0.53,  2.02]],\n",
       "\n",
       "        [[ 1.44,  0.32,  0.64, -0.18],\n",
       "         [-0.14,  1.03, -0.58,  1.57],\n",
       "         [ 0.57, -1.28, -1.04,  0.94],\n",
       "         [ 0.67,  1.83, -0.19,  1.25]],\n",
       "\n",
       "        [[ 0.95, -0.44,  0.54,  0.29],\n",
       "         [-0.15, -0.96, -0.69,  1.34],\n",
       "         [ 0.36, -0.17,  0.29, -0.32],\n",
       "         [ 2.05, -0.04,  1.38, -0.69]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = torch.randn((4,4,4 ))\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4895,  3.7981,  1.2842,  2.2062],\n",
       "         [-2.3141,  0.9513, -2.9570,  2.7279],\n",
       "         [ 0.9806, -2.5963, -0.8786, -1.0186],\n",
       "         [ 1.4202,  0.5884,  0.0212, -2.3504]]])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See below:\n",
    "\"\"\"\n",
    "| |\n",
    "| |\n",
    "| |\n",
    "| |\n",
    "\n",
    "\n",
    "Dim 0 sums in the direction of that. Move your hand in the direction of | and compress.\n",
    "\n",
    "Dim 1 sums in this direction:\n",
    "-----------\n",
    "-----------\n",
    "-----------\n",
    "\n",
    "Moved your hands in the direction of --- and clap them together. That's how it sums.\n",
    "\n",
    "Ask yourself -- in which way does that dim propagate? like is it\n",
    "row\n",
    "row\n",
    "row\n",
    "row,\n",
    "\n",
    "or is it:\n",
    "col col col col.\n",
    "\n",
    "etc...\n",
    "That's the way you sum.\n",
    "\n",
    "Remember, broadcasting is import to take int oaccount too!!!!!\n",
    "https://www.youtube.com/watch?v=PaCmpygFfXo 43:00\n",
    "\"\"\"\n",
    "\n",
    "testing.sum(0, keepdims=True) ##REMEMBERT KIDS: SUMMING BY THAT DIMENSION (e.g dim 0, the row dimension) means that row IS SUMMED!!!. So for a (27, 27), summing by dim 0 would become (1,27). The rows (downwards) are summed together!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E02\n",
    "E02: \"I was not careful with the intialization of the network in this video. \n",
    "- (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? \n",
    "- (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ah, okay. I had a bit of a look at someone else's answers and I better get the question now. Basically:\n",
    "\n",
    "\n",
    "# If our probabilities were uniform, then each character has an equal chance of being picked, i.e 1/27 probability. So it'd be \n",
    "\n",
    "torch.tensor(1/27)\n",
    "\"\"\"How do we calculate loss? (nll): We log all the probs to get nicer numbers, and we'll just get the mean of them all instead of sum (small nice number).\n",
    "# \tloss = -probs[torch.arange(32), Y].log().mean() #  Something like that. \n",
    "# In our case, all the probabilities are the same, so the mean would be whatever the log is of one of the probs. I.e\n",
    "\n",
    "-torch.tensor(1/27).log() \n",
    "\"\"\"\n",
    "loss = -torch.tensor(1/27).log() \n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to fix the munted initial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in total: 11897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 10]), torch.Size([30, 200]))"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now, what's our initialised loss?\n",
    "g = torch.Generator().manual_seed(2147483647) # This will be a 'global' \n",
    "block_size, embedding_size, num_neurons = 3, 10, 200\n",
    "parameters = generate_parameters(block_size, embedding_size, num_neurons) # ZYknow, I could just go block_size=3, embedding_size=10, ... lmao.\\\n",
    "C, W1, b1, W2, b2 = parameters\n",
    "C.shape, W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.4947, grad_fn=<NllLossBackward0>), [[], []])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = train(Xtr, Ytr, 1000, block_size, embedding_size, batch_size, parameters)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(6.3338, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3426, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_loss = evaluate_loss(Xtr, Ytr, block_size, embedding_size, parameters)\n",
    "dev_loss = evaluate_loss(Xdev, Ydev, block_size, embedding_size, parameters)\n",
    "# 24 initalised loss. god damn that's shit. Remember, just starting from a default probability distribution will give you ~3.3 loss instead of this 24 loss shit -- that's how bad our initialised weights are. We could be saving SO MANY STEPS!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay. WAIT. REGULARISATION LOSS!\n",
    "# In the end, we want the initial weights to be s.t. their probabilities are ~equal distribution.\n",
    "\n",
    "def get_probs(parameters, X): # ima just skip thee block_size etc.. in functions because we're acting like they're globals anyways.\n",
    "\t# forward pass -- on our minibatch :o !!!!\n",
    "\temb = C[X] \n",
    "\th = torch.tanh(emb.view(-1, block_size*embedding_size) @ W1 + b1)\n",
    "\tlogits = h @ W2 + b2\n",
    "\tprobs = F.softmax(logits, dim=1) # exponentiates, normalises (sum to one).\n",
    "\treturn probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22797, 27]) tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "tensor([9.0920e-02, 7.4752e-01, 3.3802e-03, 3.9280e-07, 2.7123e-06, 1.8602e-04,\n",
      "        1.1235e-05, 1.1028e-03, 6.1082e-05, 6.4050e-03, 3.5997e-05, 6.9311e-04,\n",
      "        5.1244e-02, 2.6373e-07, 6.1729e-06, 9.0667e-05, 2.0936e-05, 8.6445e-03,\n",
      "        6.2527e-02, 1.9142e-02, 7.3315e-06, 7.5664e-04, 2.0276e-03, 5.0913e-03,\n",
      "        3.7792e-05, 7.2235e-07, 8.5769e-05], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(parameters, Xdev)\n",
    "print(probs.shape, probs[0].sum())\n",
    "print(probs[0])\n",
    "# So, these probs should be way closer to each other, i.e the logits should be uniform. Hmmm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Edit: Okay, so we changed all the randn to rand, and our initial loss dropped from like ~27 to about 5. LOL. That's all we did. BRUH. \""
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # This will be a 'global' generator\n",
    "def generate_parameters(block_size, embedding_size, num_neurons):\n",
    "\t# We booosting our hidden layer size\n",
    "\tC = torch.rand(27, embedding_size, generator=g) # Generate embedding of characters, e.g (27,10)\n",
    "\n",
    "\tW1 = torch.rand((embedding_size * block_size, num_neurons), generator=g) +100 # first weights takes input from \n",
    "\tb1 = torch.rand(num_neurons, generator=g) \n",
    "\tW2 = torch.rand((num_neurons, 27), generator=g)\n",
    "\tb2 = torch.rand(27, generator=g)\n",
    "\tparameters = [C, W1, b1, W2, b2] # for easy summing parameters\n",
    "\n",
    "\tfor p in parameters:  # Turn on requires grad for our parameter matrices\n",
    "\t\tp.requires_grad = True\n",
    "\t\t\n",
    "\tprint(\"Number of parameters in total:\", sum(p.nelement() for p in parameters)) # num of parameters in total\n",
    "\n",
    "\treturn parameters\n",
    "\n",
    "\"\"\" Edit: Okay, so we changed all the randn to rand, and our initial loss dropped from like ~27 to about 5. LOL. That's all we did. BRUH. \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8a31c3d3c58cfe49314f156da5c5377d35db16854b5a4a4800d645f76ea86e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
