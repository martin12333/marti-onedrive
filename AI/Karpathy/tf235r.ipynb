{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451f0e6-d114-426e-9e7a-5b7f271727d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2247eb8e-c55f-4338-a72a-b776199312c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     D:\\conda\n",
      "jupsci_nonpip310         D:\\conda\\envs\\jupsci_nonpip310\n",
      "pip310                *  D:\\conda\\envs\\pip310\n",
      "torch_nonpip310          D:\\conda\\envs\\torch_nonpip310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819080d3-550e-4fa4-9f85-98fd8a9d7cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4e286-dffe-4608-8666-82da3a7a4736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615e646c-4f5f-4513-9f87-731f7447344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"d:/ai/my235r/runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b32f1b-6981-43f2-92f8-0e6aa9ee5544",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49282225-e2d2-4530-8e3e-cd418c2eaafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelWithLMHead,  TFAutoModel\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfxmarty/tiny-testing-gpt2-remote-code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m TFAutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfxmarty/tiny-testing-gpt2-remote-code\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead,  TFAutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fxmarty/tiny-testing-gpt2-remote-code\")\n",
    "\n",
    "model = TFAutoModel.from_pretrained(\"fxmarty/tiny-testing-gpt2-remote-code\" ) #  ,  from_pt=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "830f984e-fe05-4763-99cd-ff6183b8c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TFAutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "\n",
    "model =  TFAutoModelForCausalLM.from_pretrained(\"sshleifer/tiny-gpt2\",  from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b57d722-f32c-4e53-84b4-8fa21f24cc2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'caffe2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\conda\\envs\\pip310\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:845\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[1;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_graph(\n\u001b[0;32m    841\u001b[0m         graph(model, input_to_model, verbose, use_strict_trace)\n\u001b[0;32m    842\u001b[0m     )\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m caffe2_pb2\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_caffe2_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    848\u001b[0m         model_to_graph_def,\n\u001b[0;32m    849\u001b[0m         nets_to_graph_def,\n\u001b[0;32m    850\u001b[0m         protos_to_graph_def,\n\u001b[0;32m    851\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'caffe2'"
     ]
    }
   ],
   "source": [
    "writer.add_graph(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5bd8f26-e023-4ed9-915b-3fca7a40aab3",
   "metadata": {},
   "source": [
    "#model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "## This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.\n",
    "#model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af3678-552e-4382-b5bf-64f7295b2c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25afa5f9-5003-4067-81e9-b0a495d02a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef46844-a31a-4b6e-bccf-408ec9b74428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3882a-0f14-407d-9999-4d25f40d79ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
