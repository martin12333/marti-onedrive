{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Subword Tokenization\n\nNaturally, word tokenization can result in a massive vocabulary -- even larger if there are many unique or rare words (think specific medical or technical words). Let's say there are about 1 million unique English words and assume each word vector has 1000 dimensions. This would result in a matrix for the input layer of a neural network to have 1 million x 1000 = 1 billion weights.\n\nOne way to address this problem is to limit the vocabulary to include only the most common words in the corupus (e.g., grab 100,000 common words) and classify the others as 'unknown' with a shared unknown token. Subword tokenization is an effort to decrease the number of things we need to store while recording common subwords that may be included in some more rare or specific words.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:10:10.157868Z","iopub.execute_input":"2023-05-03T16:10:10.158403Z","iopub.status.idle":"2023-05-03T16:10:12.258562Z","shell.execute_reply.started":"2023-05-03T16:10:10.158360Z","shell.execute_reply":"2023-05-03T16:10:12.257112Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"text = 'i love cryptography, mathematics, and cybersecurity.'\ntokenized_text = tokenizer.tokenize(text)\nprint(tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:11:15.230295Z","iopub.execute_input":"2023-05-03T16:11:15.231817Z","iopub.status.idle":"2023-05-03T16:11:15.247625Z","shell.execute_reply.started":"2023-05-03T16:11:15.231753Z","shell.execute_reply":"2023-05-03T16:11:15.246401Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['i', 'love', 'crypt', '##ography', ',', 'mathematics', ',', 'and', 'cyber', '##se', '##cu', '##rity', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2Model\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2Model.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:14:39.417494Z","iopub.execute_input":"2023-05-03T16:14:39.417912Z","iopub.status.idle":"2023-05-03T16:14:41.436573Z","shell.execute_reply.started":"2023-05-03T16:14:39.417878Z","shell.execute_reply":"2023-05-03T16:14:41.435331Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"text = 'i love cryptography, mathematics, and cybersecurity.'\ntokenized_text = tokenizer.tokenize(text)\nprint(tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:14:54.982029Z","iopub.execute_input":"2023-05-03T16:14:54.983073Z","iopub.status.idle":"2023-05-03T16:14:54.997959Z","shell.execute_reply.started":"2023-05-03T16:14:54.983031Z","shell.execute_reply":"2023-05-03T16:14:54.996601Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['i', 'Ġlove', 'Ġcryptography', ',', 'Ġmathematics', ',', 'Ġand', 'Ġcybersecurity', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Token, Vector, and Embedding\n\nTo get to a point where your model can understand text, you first have to tokenize it, vectorize it and create embeddings from these vectors.\n\n- Tokenization: This is the process of dividing the original text into individual pieces called tokens. Each token is assigned a unique id to represent it as a number.\n- Vectorization: The unique ids are then assigned to randomly initialized n-dimensional vectors.\n- Embedding: To give tokens meaning, the model must be trained on them. This allows the model to learn the meanings of words and how they relate to other words. To achieve this, the word vectors are “embedded” into an embedding space. As a result, similar words should have similar vectors after training.","metadata":{}},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, BertModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:21:13.460358Z","iopub.execute_input":"2023-05-03T16:21:13.460937Z","iopub.status.idle":"2023-05-03T16:21:15.065257Z","shell.execute_reply.started":"2023-05-03T16:21:13.460884Z","shell.execute_reply":"2023-05-03T16:21:15.063830Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"text = 'I love cryptography, mathematics, and cybersecurity.'\ntokens = tokenizer.tokenize(text)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:21:17.125831Z","iopub.execute_input":"2023-05-03T16:21:17.126308Z","iopub.status.idle":"2023-05-03T16:21:17.134742Z","shell.execute_reply.started":"2023-05-03T16:21:17.126264Z","shell.execute_reply":"2023-05-03T16:21:17.133248Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['i', 'love', 'crypt', '##ography', ',', 'mathematics', ',', 'and', 'cyber', '##se', '##cu', '##rity', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"#Conver the tokens to ids\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(token_ids)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:21:30.932533Z","iopub.execute_input":"2023-05-03T16:21:30.932984Z","iopub.status.idle":"2023-05-03T16:21:30.940130Z","shell.execute_reply.started":"2023-05-03T16:21:30.932937Z","shell.execute_reply":"2023-05-03T16:21:30.938653Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[1045, 2293, 19888, 9888, 1010, 5597, 1010, 1998, 16941, 3366, 10841, 15780, 1012]\n","output_type":"stream"}]},{"cell_type":"code","source":"#We can go backwards from ids to tokens\nprint(tokenizer.convert_ids_to_tokens(token_ids))","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:21:58.569196Z","iopub.execute_input":"2023-05-03T16:21:58.569656Z","iopub.status.idle":"2023-05-03T16:21:58.576597Z","shell.execute_reply.started":"2023-05-03T16:21:58.569616Z","shell.execute_reply":"2023-05-03T16:21:58.574941Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"['i', 'love', 'crypt', '##ography', ',', 'mathematics', ',', 'and', 'cyber', '##se', '##cu', '##rity', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"#There is a built in encode function that also does this.\ntoken_ids = tokenizer.encode(text)\nprint(token_ids)\n#The encode function translate the token with 101 to the specail [CLS] token which represents the beggining of the sequence <BOS> token. The token with id 102 is the end of the sequence (<EOS>) token.\ntokens_from_encode = tokenizer.convert_ids_to_tokens(token_ids)\nprint(tokens_from_encode)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:22:11.973914Z","iopub.execute_input":"2023-05-03T16:22:11.974377Z","iopub.status.idle":"2023-05-03T16:22:11.982089Z","shell.execute_reply.started":"2023-05-03T16:22:11.974338Z","shell.execute_reply":"2023-05-03T16:22:11.980530Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[101, 1045, 2293, 19888, 9888, 1010, 5597, 1010, 1998, 16941, 3366, 10841, 15780, 1012, 102]\n['[CLS]', 'i', 'love', 'crypt', '##ography', ',', 'mathematics', ',', 'and', 'cyber', '##se', '##cu', '##rity', '.', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Vectorization\n\nTo obtain an embedding for a token, you first need to create a model. The following will download the pre-trained bert-base-uncased model with its weights and its embeddings.","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:41:52.848428Z","iopub.execute_input":"2023-05-03T16:41:52.850507Z","iopub.status.idle":"2023-05-03T16:41:52.858635Z","shell.execute_reply.started":"2023-05-03T16:41:52.850451Z","shell.execute_reply":"2023-05-03T16:41:52.857237Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# get the embedding vector for the word \"cyber\" or whatever example you want to use!\nexample_word = \"cyber\"\nexample_token_id = tokenizer.convert_tokens_to_ids([example_word])[0]\nexample_embedding = model.embeddings.word_embeddings(torch.tensor([example_token_id]))\n\nprint(example_embedding.shape)\nprint(example_embedding)\n# torch.Size([1, 768])\n# The returned word vector has a size of 768 dimensions, the same as the BERT model. ","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:42:05.994801Z","iopub.execute_input":"2023-05-03T16:42:05.995249Z","iopub.status.idle":"2023-05-03T16:42:06.050736Z","shell.execute_reply.started":"2023-05-03T16:42:05.995192Z","shell.execute_reply":"2023-05-03T16:42:06.049129Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"torch.Size([1, 768])\ntensor([[-3.9401e-02, -3.4605e-02, -3.0109e-02, -5.4147e-02,  6.7955e-02,\n          8.1688e-02, -4.9087e-02, -8.6033e-03, -2.2622e-02, -3.3102e-02,\n          3.3617e-02, -3.2629e-02, -5.9386e-02, -5.0595e-02, -5.0741e-02,\n          9.8050e-03, -9.8762e-02, -3.0505e-02,  1.6212e-02, -9.1320e-02,\n         -1.9809e-02, -7.5010e-02, -8.5134e-03,  2.1966e-02, -7.5120e-02,\n         -7.9889e-02,  2.4941e-03, -5.2005e-02, -7.0397e-02, -1.2168e-03,\n          7.5779e-02,  1.8078e-02, -1.6702e-04, -7.9205e-03,  1.4067e-02,\n         -3.7053e-02, -5.6451e-02, -7.7005e-02, -9.4111e-02, -7.0676e-03,\n          8.8820e-03,  1.0885e-02, -3.4853e-02, -4.3510e-02,  1.8766e-02,\n         -2.2594e-02, -5.9443e-02, -4.7548e-02, -3.8350e-02,  5.0268e-03,\n         -8.0528e-03, -8.3088e-03, -4.2568e-03,  6.0233e-02, -3.2517e-02,\n         -2.4433e-02,  5.7999e-02, -2.1103e-02, -9.5786e-03, -1.3551e-02,\n         -7.1807e-03,  4.9844e-02,  5.2372e-02, -6.0588e-02, -4.2348e-02,\n          2.8226e-02, -8.5436e-03, -3.5105e-02,  3.2788e-02, -1.5899e-02,\n         -4.1088e-02, -8.2448e-04, -6.9428e-02, -9.0960e-02, -2.6461e-02,\n         -1.7887e-02,  7.4146e-04, -2.7913e-02, -3.1940e-02, -8.1959e-02,\n         -1.4205e-02, -2.6640e-02,  4.8882e-04, -4.4074e-02, -8.7136e-02,\n          3.5806e-02, -5.6032e-02,  3.5647e-03, -3.2017e-02, -3.5369e-02,\n         -1.4800e-02, -1.3298e-02,  2.8593e-02, -1.8064e-02, -4.3759e-02,\n         -2.7879e-02,  4.1462e-02, -5.9201e-02, -8.7744e-02, -2.6463e-02,\n          2.4502e-02, -2.5820e-02,  2.3341e-02,  1.6670e-03, -3.5408e-02,\n         -1.1843e-02, -3.7509e-02, -7.3677e-02,  1.9059e-02, -2.6162e-02,\n          3.8854e-03, -4.3898e-02, -1.0172e-01,  5.6484e-03, -7.3701e-03,\n         -3.8452e-02, -3.1892e-02, -1.6095e-02, -5.6213e-02, -3.9976e-02,\n          1.7792e-02,  1.3026e-01, -7.3905e-03,  1.9183e-02, -6.6167e-02,\n         -1.7978e-02, -3.7177e-02, -7.6274e-02, -2.1983e-02, -1.1646e-01,\n          3.4786e-02,  1.1191e-03,  2.9892e-02, -2.8203e-02, -1.4341e-02,\n         -7.0218e-02,  1.4244e-02,  2.3061e-02, -7.0371e-02,  1.8647e-02,\n         -9.5140e-02, -5.4119e-02,  5.9538e-02, -5.6479e-03, -1.0947e-01,\n         -6.6462e-02, -5.9338e-02, -5.8991e-02, -1.5361e-01, -8.3703e-02,\n         -6.2723e-02, -2.3901e-02, -9.3860e-02, -8.7310e-02,  5.9136e-03,\n         -1.1333e-01, -2.9372e-02, -6.7188e-02, -3.4656e-02, -7.1936e-03,\n         -9.4189e-02, -6.4157e-02, -4.0340e-02, -2.2044e-02,  6.2065e-02,\n         -5.9834e-03, -5.9912e-02, -1.8335e-02,  2.1835e-02,  7.0201e-03,\n         -3.2165e-02, -5.0260e-02, -5.1034e-03,  1.3696e-02, -2.6898e-02,\n          4.6841e-02, -1.2378e-02, -5.0511e-02, -3.8794e-02, -6.4513e-02,\n         -3.1136e-02, -1.4018e-02, -1.3014e-02, -2.4286e-02,  1.6328e-03,\n         -1.7174e-02,  9.1587e-03,  1.9308e-02, -5.2222e-02, -6.6952e-03,\n         -4.8338e-02, -1.2950e-02, -3.5663e-02, -2.9376e-02, -4.6838e-02,\n         -1.9198e-02, -2.0405e-02, -1.3359e-02, -8.0129e-02, -4.1956e-02,\n         -1.6175e-02, -6.8845e-02, -8.6537e-02, -6.5955e-02, -8.4280e-02,\n         -1.6486e-02, -6.8726e-02,  1.6747e-03,  9.0168e-03, -4.1431e-02,\n         -5.5600e-02, -6.2546e-02, -5.3420e-02,  4.3178e-03,  1.5885e-02,\n          4.4710e-02, -8.6187e-02, -1.1944e-02,  2.5243e-02, -9.2871e-02,\n          5.4074e-02,  3.9276e-02, -7.0013e-02,  5.5179e-03, -5.8905e-03,\n          1.3431e-02, -6.0944e-02, -8.0892e-02, -4.2452e-02, -3.9259e-02,\n         -3.8166e-02, -8.9411e-02, -2.4222e-02,  3.6514e-03, -3.7169e-02,\n         -6.4083e-02, -9.7830e-02,  4.0016e-03, -4.9076e-02, -1.1070e-01,\n         -1.4543e-03, -2.9315e-02, -4.0586e-02, -9.3604e-02, -5.0869e-02,\n          3.1180e-02, -4.0017e-03, -5.5100e-02, -2.3266e-02, -1.6988e-01,\n         -9.5345e-02, -1.0813e-01, -6.2133e-02, -6.1176e-02, -9.6633e-02,\n         -4.1319e-03, -2.5814e-02,  2.0984e-02, -4.9929e-02, -1.3637e-02,\n         -8.0177e-02, -4.3269e-02,  2.4789e-02, -2.0787e-02, -1.1123e-01,\n         -3.4344e-02,  4.3096e-03,  4.3673e-03,  1.0061e-02, -4.7653e-02,\n         -4.0021e-02, -4.3819e-02, -2.2537e-02, -5.9910e-03, -2.8874e-02,\n         -3.0730e-02,  1.0239e-02, -6.1657e-02, -2.5575e-02,  2.4877e-02,\n          2.7819e-02,  2.6311e-02, -5.6247e-02,  6.4405e-02, -4.0403e-02,\n         -1.4306e-01, -1.1694e-02,  3.1532e-03, -4.9626e-02, -6.8332e-02,\n         -4.4865e-02,  3.0035e-02, -4.7237e-02, -1.0500e-01,  3.0536e-02,\n          6.0560e-03, -5.7305e-02, -8.5304e-02, -6.4057e-02,  1.0789e-02,\n         -3.0325e-02,  4.3752e-02, -6.6151e-02, -6.1894e-02, -9.5252e-02,\n         -3.1778e-02, -2.4578e-02, -4.9010e-02,  2.2882e-03,  6.1102e-02,\n          4.1833e-02,  2.4025e-02, -2.4262e-02, -9.8678e-02, -4.7875e-02,\n          1.1427e-02, -5.5130e-02, -6.8145e-02, -7.4004e-02,  2.0184e-02,\n          3.1230e-02, -6.5334e-02, -5.1223e-02, -4.7555e-03, -4.4730e-03,\n         -8.5940e-02, -6.9796e-02, -1.2212e-01,  4.1233e-02, -4.7209e-02,\n          4.6662e-03, -2.8376e-02, -2.7012e-02,  5.4581e-02, -3.3836e-02,\n         -8.1522e-03, -2.0827e-02,  5.9217e-02, -5.2342e-02, -2.1569e-02,\n         -1.7639e-02, -3.8038e-02, -8.7946e-02,  1.1035e-02,  1.8354e-02,\n          1.0031e-01, -2.9169e-02, -5.4888e-02, -2.9060e-02, -2.0084e-02,\n          5.8775e-02, -9.4766e-02,  5.3030e-03,  2.3630e-02, -1.3374e-02,\n          1.8673e-02,  3.4932e-02, -7.1373e-03,  1.6878e-02, -3.9657e-02,\n          5.7239e-03, -9.6378e-02, -9.3218e-03, -3.7040e-02, -3.2130e-02,\n         -2.8907e-02, -5.4565e-02, -4.5882e-02,  9.7864e-03, -2.6923e-02,\n         -2.5985e-02, -4.0325e-02, -1.1896e-02, -1.7820e-02, -4.0449e-02,\n         -8.5701e-02,  3.1718e-02, -8.7777e-02, -4.3137e-03,  5.4914e-02,\n         -5.1281e-02, -9.0411e-02, -1.6441e-02, -3.1447e-02, -5.0982e-02,\n          4.4513e-03, -7.0131e-02, -3.9784e-02,  8.3009e-03, -1.2452e-03,\n         -1.7297e-02, -3.6553e-02, -1.0423e-01, -7.9019e-02,  5.0225e-02,\n         -4.1457e-02,  2.1611e-02, -4.6645e-02,  1.5932e-02, -1.0038e-01,\n          2.9985e-02, -1.4547e-01, -1.0189e-01, -1.5040e-02, -5.1653e-02,\n          2.5232e-02, -6.0700e-02, -3.1773e-02,  6.1327e-02, -1.9653e-02,\n         -6.1909e-02, -5.4762e-03,  3.7552e-02,  6.3886e-03,  4.3107e-02,\n         -4.5427e-02, -1.0191e-01, -1.9916e-02, -5.1439e-02, -6.0400e-02,\n         -9.1153e-02, -6.3307e-02, -2.6460e-02, -8.1289e-02, -1.9112e-02,\n         -3.0121e-02, -3.0234e-02, -4.8373e-02,  5.7349e-02, -6.4893e-02,\n          1.3848e-02,  3.7282e-03, -8.1435e-02, -6.4722e-02, -5.9237e-02,\n          4.2877e-02,  8.4145e-03, -3.1018e-04, -6.2850e-02, -3.5931e-02,\n         -3.4545e-02, -3.7055e-02, -4.4848e-02,  3.3064e-02, -6.2813e-02,\n         -8.6538e-02,  2.5934e-02, -1.5234e-02,  2.7184e-02, -9.1016e-02,\n         -1.9443e-02, -1.0442e-02, -2.1128e-02,  1.8830e-02, -2.1891e-02,\n         -6.0705e-02, -1.1077e-02,  3.5361e-02, -1.1473e-02, -9.7567e-02,\n          8.9449e-03,  8.1097e-02, -2.4679e-02,  8.4703e-03,  2.0698e-04,\n         -5.4690e-02, -2.7535e-02, -1.2343e-02,  1.4027e-02,  2.4663e-03,\n          1.3588e-02, -9.0395e-02, -1.6741e-01, -1.8476e-02,  3.6762e-02,\n          2.3374e-02, -5.7774e-02, -2.1562e-02, -4.2636e-02, -1.3699e-02,\n          7.5861e-04, -5.1254e-02,  6.7341e-03, -2.8799e-02, -2.0258e-02,\n          3.8283e-03,  1.5367e-02, -8.7340e-02, -5.1273e-02, -4.0683e-02,\n          5.8980e-03, -3.0737e-02, -5.1467e-03, -6.3173e-02, -6.2981e-02,\n          2.1278e-02, -1.0655e-01, -6.0756e-02, -1.9722e-02, -3.4648e-02,\n         -4.3190e-02, -1.6945e-02, -4.8938e-03, -3.3389e-02, -2.2459e-02,\n         -3.4718e-02, -5.0213e-03, -5.7528e-02, -1.3041e-01, -2.7888e-02,\n         -4.0453e-02, -5.0445e-02, -2.5196e-02,  7.3883e-02,  1.1775e-02,\n         -5.5046e-02, -2.0090e-02,  3.7248e-02, -3.4304e-02, -3.2013e-02,\n         -8.5248e-02, -6.8621e-02, -5.0401e-02, -7.8058e-02, -6.1862e-02,\n         -3.4873e-02, -1.8657e-02,  1.6678e-02, -5.2402e-02,  3.4359e-02,\n         -5.6606e-02, -4.6724e-02, -1.6149e-02, -2.1784e-02, -8.3353e-03,\n         -4.2680e-02,  4.5969e-04, -8.9230e-02, -3.3357e-02, -8.9991e-02,\n          6.3683e-02,  3.9213e-02,  2.6578e-03, -6.4170e-02,  4.4926e-02,\n          5.2378e-02,  5.2344e-03, -2.7726e-02, -3.8151e-02, -6.9672e-02,\n         -2.6377e-02, -8.8637e-02, -5.9777e-02,  4.3164e-02, -6.9835e-02,\n         -4.9730e-02, -5.7806e-02, -4.4083e-02, -3.4030e-02, -6.1979e-02,\n         -2.9313e-02,  3.7077e-02, -2.9917e-02, -9.1744e-02, -4.0409e-02,\n         -1.8455e-02, -6.8673e-03, -5.2790e-02, -2.9778e-02, -1.5794e-03,\n         -4.5328e-03, -1.0599e-01, -1.2429e-02, -2.7613e-03, -4.4569e-02,\n          2.2149e-02,  7.7332e-03,  1.2970e-01,  4.5980e-02, -8.7791e-02,\n         -3.5002e-02, -3.6988e-02, -1.3252e-01,  2.6264e-02, -5.2492e-02,\n         -1.2719e-02, -7.7309e-02, -7.7770e-02, -1.3921e-02, -2.5879e-02,\n         -2.8626e-02, -6.0422e-02, -3.2365e-02, -6.8408e-02,  3.6379e-02,\n          5.0257e-03, -4.3717e-02, -1.7189e-02, -6.6315e-02, -1.1382e-02,\n          2.0267e-02, -8.2910e-03, -3.5558e-02, -2.1809e-02,  2.5554e-02,\n         -2.0842e-03, -5.9665e-02, -2.4560e-03, -3.6077e-02,  4.2977e-03,\n          2.3317e-02, -6.3316e-02, -4.3204e-03,  2.0467e-03,  8.9947e-02,\n         -4.6216e-02, -3.1159e-02, -6.0922e-02, -8.4711e-02, -5.3305e-02,\n         -1.3819e-01, -3.3572e-02, -4.0197e-02, -2.3342e-02,  5.1589e-02,\n         -5.9938e-02, -2.7392e-02, -1.0066e-02, -7.0285e-02, -5.5063e-03,\n         -4.5843e-02, -1.9061e-02, -1.9151e-02, -3.5170e-02, -2.0972e-02,\n         -5.1583e-02, -4.2541e-02, -3.8717e-02,  3.9891e-02,  1.8224e-02,\n         -2.4561e-02, -5.7083e-02, -6.0432e-02, -1.9548e-02,  4.8211e-02,\n         -4.9000e-03, -1.0236e-02, -3.8559e-03, -4.8519e-02,  1.6515e-02,\n         -4.6999e-02, -1.8101e-02, -5.4040e-02, -5.3881e-02,  1.0059e-01,\n         -2.6259e-02, -2.3405e-02, -9.7366e-02, -2.7935e-02, -6.2200e-02,\n         -3.9237e-02, -5.1470e-02,  9.0777e-03,  7.8190e-03, -2.0461e-02,\n         -1.6845e-02, -1.1518e-01, -5.4610e-02, -5.7100e-02, -1.0083e-01,\n          8.4252e-02, -6.5940e-02,  1.4139e-02, -1.6771e-02, -2.2184e-02,\n         -1.8915e-02, -1.2850e-02, -2.7364e-02, -2.0521e-02,  7.5342e-03,\n         -4.5615e-02,  1.6827e-02, -1.2102e-02,  3.3765e-02, -1.0319e-01,\n         -2.5683e-02, -3.1546e-02, -1.0766e-01, -1.2148e-01,  7.5217e-03,\n          3.8224e-03, -9.9011e-02,  1.3561e-02,  6.6480e-02, -3.6002e-02,\n          5.8238e-02,  1.5591e-03, -1.0392e-02, -4.5404e-02, -1.2518e-02,\n         -5.2142e-02,  3.6303e-03, -7.4569e-02, -7.6558e-02, -1.0322e-02,\n          3.2944e-02,  3.1301e-02,  7.3576e-03, -4.3768e-02, -7.1951e-02,\n          3.8777e-02, -1.8392e-02,  6.7178e-03, -6.2555e-02, -6.2834e-03,\n         -6.1032e-02, -6.4446e-03, -3.8156e-02,  6.1784e-03,  3.9252e-02,\n         -1.9847e-02,  3.7527e-02, -4.7370e-02, -4.2607e-02,  5.1530e-02,\n         -7.0815e-02,  3.5483e-02, -6.5120e-04, -8.0696e-03, -4.9978e-02,\n         -2.7591e-02, -6.9052e-02, -7.4689e-02, -1.8963e-02, -3.7351e-02,\n          2.6503e-02, -8.0955e-03, -6.3272e-03,  3.7753e-02, -9.4383e-04,\n          1.5052e-03,  3.3337e-02, -4.7768e-02, -6.8369e-02,  3.8199e-02,\n         -2.6181e-02, -2.8629e-02, -2.0714e-02, -5.5663e-02,  6.6296e-03,\n         -3.4100e-02, -7.5004e-02, -6.3511e-02, -5.8473e-02, -5.5523e-02,\n         -2.5006e-02, -4.2293e-02,  2.1563e-02, -3.5828e-02,  1.9288e-02,\n          1.2599e-02,  4.8404e-03,  7.9362e-03, -6.3790e-02, -2.3316e-02,\n          1.0659e-02, -2.1335e-02, -8.1709e-02]], grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Cosine similarity\nCosine similarity is a way to measure how similar two things are. It’s often used in natural language processing to compare the content of two texts.\n\nTo calculate the cosine similarity, we look at the angle between two vectors. If the vectors point in the same direction, they are more similar, and if they point in opposite directions, they are less similar. The result is a number between -1 and 1, where 1 means the vectors are identical and -1 means they are completely different.\n\n## Euclidean Distance\nEuclidean Distance is another way to measure how similar two things are. To calculate the Euclidean Distance, d(u,v), we compute the length, |u - v|. The result is non-negative number, where 0 means the vectors are identical and the larger the number the further they are away from each other.","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:43:58.784609Z","iopub.execute_input":"2023-05-03T16:43:58.785090Z","iopub.status.idle":"2023-05-03T16:43:58.791790Z","shell.execute_reply.started":"2023-05-03T16:43:58.785053Z","shell.execute_reply":"2023-05-03T16:43:58.790077Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"word_one = \"king\"\nword_two = \"queen\"\none_token_id = tokenizer.convert_tokens_to_ids([word_one])[0]\none_embedding = model.embeddings.word_embeddings(torch.tensor([one_token_id]))\n\ntwo_token_id = tokenizer.convert_tokens_to_ids([word_two])[0]\ntwo_embedding = model.embeddings.word_embeddings(torch.tensor([two_token_id]))\n\ncos = torch.nn.CosineSimilarity(dim=1)\ncosine_similarity = cos(one_embedding, two_embedding)\neuclidean_distance = torch.cdist(one_embedding,two_embedding)\nprint(f'Cosine Similarity between \\'{word_one}\\' and \\'{word_two}\\': {cosine_similarity[0]}')\n# 0.646\nprint(f'Euclidean Distance between \\'{word_one}\\' and \\'{word_two}\\': {euclidean_distance[0][0]}')\n# 0.886","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:44:26.374303Z","iopub.execute_input":"2023-05-03T16:44:26.374756Z","iopub.status.idle":"2023-05-03T16:44:26.420432Z","shell.execute_reply.started":"2023-05-03T16:44:26.374716Z","shell.execute_reply":"2023-05-03T16:44:26.419309Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Cosine Similarity between 'king' and 'queen': 0.6468513011932373\nEuclidean Distance between 'king' and 'queen': 0.8862659335136414\n","output_type":"stream"}]},{"cell_type":"code","source":"word_three = \"man\"\nword_four = \"woman\"\nthree_token_id = tokenizer.convert_tokens_to_ids([word_three])[0]\nthree_embedding = model.embeddings.word_embeddings(torch.tensor([three_token_id]))\n\nfour_token_id = tokenizer.convert_tokens_to_ids([word_four])[0]\nfour_embedding = model.embeddings.word_embeddings(torch.tensor([four_token_id]))\n\ncos = torch.nn.CosineSimilarity(dim=1)\ncosine_similarity = cos(three_embedding, four_embedding)\neuclidean_distance = torch.cdist(three_embedding,four_embedding)\nprint(f'Cosine Similarity between \\'{word_three}\\' and \\'{word_four}\\': {cosine_similarity[0]}')\n# 0.633\nprint(f'Euclidean Distance between \\'{word_three}\\' and \\'{word_four}\\': {euclidean_distance[0][0]}')\n# 0.847","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:44:45.102843Z","iopub.execute_input":"2023-05-03T16:44:45.103291Z","iopub.status.idle":"2023-05-03T16:44:45.114857Z","shell.execute_reply.started":"2023-05-03T16:44:45.103253Z","shell.execute_reply":"2023-05-03T16:44:45.113538Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Cosine Similarity between 'man' and 'woman': 0.6337041854858398\nEuclidean Distance between 'man' and 'woman': 0.8479673266410828\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}