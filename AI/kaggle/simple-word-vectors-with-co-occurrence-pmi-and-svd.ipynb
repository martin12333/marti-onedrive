{"cells":[{"metadata":{"_uuid":"1c1d9246a826785ad74accf0bacaeaf2f44a7d4c","_cell_guid":"b8fe67d9-06b6-4dff-84f3-8d90ddf13f7e"},"cell_type":"markdown","source":"This is an implementation of \"word vectors\" based on Chris Moody's blog post: http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/\n\nCompared to other popular approaches this is very simple and computational inexpensive but still yields meaningful representations.\n\nThe steps are described very well in Chris' post so I'll go straight to the implementation.\n\n**Scroll to the bottom if you just want to see some nearest-neighbor examples.**"},{"metadata":{"_uuid":"bd36a41066ff93e87453ec02db1ea01deeb75f2d","_cell_guid":"52de9db7-3a8c-45a0-a931-a641e452fbb9","trusted":true,"collapsed":true},"cell_type":"code","source":"from __future__ import print_function, division\nfrom collections import Counter\nfrom itertools import combinations\nfrom math import log\nfrom pprint import pformat\nfrom scipy.sparse import csc_matrix\nfrom scipy.sparse.linalg import svds\nfrom string import punctuation\nfrom time import time\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nprint('Ready')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b1ff6b6132a643b0af01b8ed1cf7cf4a6abe08e","_cell_guid":"e09e7fc7-8067-4b44-bccb-7bcba2acab6f","trusted":true,"collapsed":true},"cell_type":"code","source":"# 1. Read and preprocess titles from HN posts.\npunctrans = str.maketrans(dict.fromkeys(punctuation))\ndef tokenize(title):\n    x = title.lower() # Lowercase\n    x = x.encode('ascii', 'ignore').decode() # Keep only ascii chars.\n    x = x.translate(punctrans) # Remove punctuation\n    return x.split() # Return tokenized.\n\nt0 = time()\ndf = pd.read_csv('../input/HN_posts_year_to_Sep_26_2016.csv', usecols=['title'])\ntexts_tokenized = df['title'].apply(tokenize)\nprint(texts_tokenized[:10])\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(df)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc98825b0e6d2043eb16d72b6d5f037a1b6f4504","_cell_guid":"102e4461-2249-4972-929d-a22b6eb9fc0f","trusted":true,"collapsed":true},"cell_type":"code","source":"# 2a. Compute unigram and bigram counts.\n# A unigram is a single word (x). A bigram is a pair of words (x,y).\n# Bigrams are counted for any two terms occurring in the same title.\n# For example, the title \"Foo bar baz\" has unigrams [foo, bar, baz]\n# and bigrams [(bar, foo), (bar, baz), (baz, foo)]\nt0 = time()\ncx = Counter()\ncxy = Counter()\nfor text in texts_tokenized:\n    \n    for x in text:\n        cx[x] += 1\n\n    # Count all pairs of words, even duplicate pairs.\n    for x, y in map(sorted, combinations(text, 2)):\n        cxy[(x, y)] += 1\n\n#     # Alternative: count only 2-grams.\n#     for x, y in zip(text[:-1], text[1:]):\n#         cxy[(x, y)] += 1\n\n#     # Alternative: count all pairs of words, but don't double count.\n#     for x, y in set(map(tuple, map(sorted, combinations(text, 2)))):\n#         cxy[(x,y)] += 1\n\nprint('%.3lf seconds (%.5lf / iter)' %\n      (time() - t0, (time() - t0) / len(texts_tokenized)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"561f177ea5a9e45558cc7a4b9d924dc4e7279e84","_cell_guid":"eeb6ef79-5747-404c-a3eb-2a8f8afca4fc","trusted":true,"collapsed":true},"cell_type":"code","source":"# 2b. Remove frequent and infrequent unigrams.\n# Pick arbitrary occurrence count thresholds to eliminate unigrams occurring\n# very frequently or infrequently. This decreases the vocab size substantially.\nprint('%d tokens before' % len(cx))\nt0 = time()\nmin_count = (1 / 1000) * len(df)\nmax_count = (1 / 50) * len(df)\nfor x in list(cx.keys()):\n    if cx[x] < min_count or cx[x] > max_count:\n        del cx[x]\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))\nprint('%d tokens after' % len(cx))\nprint('Most common:', cx.most_common()[:25])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"971abff6c59afd7f2b6c159ad38e0911afd905e5","_cell_guid":"c2f26fc4-0867-4a6b-9165-f1fd0b213b09","trusted":true,"collapsed":true},"cell_type":"code","source":"# 2c. Remove frequent and infrequent bigrams.\n# Any bigram containing a unigram that was removed must now be removed.\nt0 = time()\nfor x, y in list(cxy.keys()):\n    if x not in cx or y not in cx:\n        del cxy[(x, y)]\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35020b2f5f97d504d6f12e57530bf692d9d10fd0","_cell_guid":"2624f811-19f3-4a3f-bd27-93d6bd5041bd","trusted":true,"collapsed":true},"cell_type":"code","source":"# 3. Build unigram <-> index lookup.\nt0 = time()\nx2i, i2x = {}, {}\nfor i, x in enumerate(cx.keys()):\n    x2i[x] = i\n    i2x[i] = x\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68a693d75730777740beddecd1363dfde4cb97a8","_cell_guid":"00442677-9d7c-4e8c-a990-9046265aa1c2","trusted":true,"collapsed":true},"cell_type":"code","source":"# 4. Sum unigram and bigram counts for computing probabilities.\n# i.e. p(x) = count(x) / sum(all counts).\nt0 = time()\nsx = sum(cx.values())\nsxy = sum(cxy.values())\nprint('%.3lf seconds (%.5lf / iter)' %\n      (time() - t0, (time() - t0) / (len(cx) + len(cxy))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cee3977bc3623795a2d17a8d914902b03477e5d","_cell_guid":"c9fa5b0c-c26d-4d0e-a652-ee0e6caf22d0","trusted":true,"collapsed":true},"cell_type":"code","source":"# 5. Accumulate data, rows, and cols to build sparse PMI matrix\n# Recall from the blog post that the PMI value for a bigram with tokens (x, y) is: \n# PMI(x,y) = log(p(x,y) / p(x) / p(y)) = log(p(x,y) / (p(x) * p(y)))\n# The probabilities are computed on the fly using the sums from above.\nt0 = time()\npmi_samples = Counter()\ndata, rows, cols = [], [], []\nfor (x, y), n in cxy.items():\n    rows.append(x2i[x])\n    cols.append(x2i[y])\n    data.append(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)))\n    pmi_samples[(x, y)] = data[-1]\nPMI = csc_matrix((data, (rows, cols)))\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))\nprint('%d non-zero elements' % PMI.count_nonzero())\nprint('Sample PMI values\\n', pformat(pmi_samples.most_common()[:10]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74bc7b7db97854c092e9c9a4dbedfa1978fba773","_cell_guid":"71729529-e255-4840-aa37-40c3fb5da450","trusted":true,"collapsed":true},"cell_type":"code","source":"# 6. Factorize the PMI matrix using sparse SVD aka \"learn the unigram/word vectors\".\n# This part replaces the stochastic gradient descent used by Word2vec\n# and other related neural network formulations. We pick an arbitrary vector size k=20.\nt0 = time()\nU, _, _ = svds(PMI, k=20)\nprint('%.3lf seconds' % (time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdc1ca820c5c1e5fae06fcc90b2fd88bc2f33e52","_cell_guid":"a977236a-f5e9-43a3-aed1-895046f164de","trusted":true,"collapsed":true},"cell_type":"code","source":"# 7. Normalize the vectors to enable computing cosine similarity in next cell.\n# If confused see: https://en.wikipedia.org/wiki/Cosine_similarity#Definition\nt0 = time()\nnorms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\nU /= np.maximum(norms, 1e-7)\nprint('%.3lf seconds' % (time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dd5b44c3dcba9239ac98085f706de3c35f85112","_cell_guid":"5d8257bb-24a0-465d-83eb-f7c3f5bac6a6","trusted":true,"collapsed":true},"cell_type":"code","source":"# 8. Show some nearest neighbor samples as a sanity-check.\n# The format is <unigram> <count>: (<neighbor unigram>, <similarity>), ...\n# From this we can see that the relationships make sense.\nk = 5\nfor x in ['facebook', 'twitter', 'instagram', 'messenger', 'hack', 'security', \n          'deep', 'encryption', 'cli', 'venture', 'paris']:\n    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.\n    s = ''\n    # Compile the list of nearest neighbor descriptions.\n    # Argpartition is faster than argsort and meets our needs.\n    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:\n        if i2x[i] == x: continue\n        xy = tuple(sorted((x, i2x[i])))\n        s += '(%s, %.3lf) ' % (i2x[i], dd[i])\n    print('%s, %d\\n %s' % (x, cx[x], s))\n    print('-' * 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21bd4a5813b03feb0da800aec1fee1848b346b60"},"cell_type":"markdown","source":"@driesssens noticed an \"Alphabetical bias\" in the nearest neighbors and emailed me about it. This means that nearest neighbor words are more likely to start with the same letter compared to a random sample of words. This makes some sense given the propensity for abbreviations and acronyms in tech article titles, but I was surprised how extreme the bias is. The next couple cells measure this by comparing the number of unique first characters in a sample of 10 random words to the number of unique first characters when computing nearest neighbors. \n\n@driesssens has some more analysis on this kernel: https://www.kaggle.com/driesssens/pmi-and-svd-word-vectors-with-alphabetical-bias"},{"metadata":{"trusted":true,"_uuid":"c8e63c551290b41ff828c79defe7063ae20e10f8","collapsed":true},"cell_type":"code","source":"# 9. Measure the alphabetical bias in the word vectors. i.e., the nearest neighbors are more likely to start with the same letter.\n# 9a. Take random samples of k words and count the number of distinct first characters. Plot the frequencies.\nwords = list(cx.keys())\nk = 10\nN = 10000\ndistinct_first_char_counts = [0] * (k + 1)\nfor _ in range(N):\n    first_chars = set()\n    for word in np.random.choice(words, size=k):\n        first_chars.add(word[0])\n    distinct_first_char_counts[len(first_chars)] += 1\n\nplt.title(\"Distinct first char counts (random sample)\")\nplt.bar(np.arange(k + 1), distinct_first_char_counts)\nplt.xlabel(\"Number of unique characters\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"477e52d8cf1d975bdc3079401c4518c74226fb2a","collapsed":true},"cell_type":"code","source":"# 9b. Take nearest neighbor samples of k words (including the query word) and count the number of distinct first characters.\n# The plot shows that there is clearly an alphabetical bias for nearest neighbors which didn't exist for random samples.\ndistinct_first_char_counts = [0] * (k + 1)\nfor x in np.random.choice(words, size=N):\n    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.\n    first_chars = set()\n    for i in np.argpartition(-1 * dd, k + 1)[:k]:\n        first_chars.add(i2x[i][0])\n    distinct_first_char_counts[len(first_chars)] += 1\n\nplt.title(\"Distinct first char counts (nearest neighbors)\")\nplt.bar(np.arange(k + 1), distinct_first_char_counts)\nplt.xlabel(\"Number of unique characters\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}