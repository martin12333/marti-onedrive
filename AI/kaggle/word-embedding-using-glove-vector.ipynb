{"cells":[{"metadata":{"_uuid":"c1ce6d19d90b9ae204a389894bf6e74390e93d7e","_cell_guid":"8f13803a-e8aa-4c89-ad97-b0fab0abd6ea","trusted":false,"collapsed":true},"cell_type":"code","source":"!wget \"humancomputation.com/blog/wp-content/uploads/2016/11/Experiment.png\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d25e79a32c698992819436fc55ad7fd8413733fd","_cell_guid":"93e3dbd3-27bd-4a54-849d-f4c838d62de2"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"7e13d5db115c297d8be10d93a8503f7bc7e52349","_cell_guid":"f4978f74-d012-4762-bd74-6ec2422795cf"},"cell_type":"markdown","source":"## Word Embedding Magic\nI just saw this data and realized this is 50-dimensional GloVe vectors represention of words. In this notebook, I will try to show some magic of this dataset and the importance this dataset should deserve!\n![](https://humancomputation.com/blog/wp-content/uploads/2016/11/Experiment.png)\n\nPic Credit : https://humancomputation.com\n\n"},{"metadata":{"_uuid":"921e2fef7df44f1a5e2754244aee261680cf2d59","_cell_guid":"47e299fe-d2e3-4d45-b870-325084501307"},"cell_type":"markdown","source":"**Introduction**\n\nWord embedding plays an important in Natural language processing. Throwing the one hot vector representation out of the window this feature learning maps words or phrases from the vocabulary to vectors of real numbers. GloVe is one of the approach where each word is mapped to 50-dimension vector. These vector can be used to learn the semantic of words like Man is Woman as King is to Queen. Or Man + Female = Woman. This embedding plays an important role in many applications. It is kind of a transfer learning also where these embedding are leart from large corpus of data and then can be used on smaller dataset.\n"},{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"def read_data(file_name):\n    with open(file_name,'r') as f:\n        word_vocab = set() # not using list to avoid duplicate entry\n        word2vector = {}\n        for line in f:\n            line_ = line.strip() #Remove white space\n            words_Vec = line_.split()\n            word_vocab.add(words_Vec[0])\n            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n    print(\"Total Words in DataSet:\",len(word_vocab))\n    return word_vocab,word2vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eb5c909c46ef4ffde74ae480c7f36750704711d","_cell_guid":"3dc95d14-8634-4bfb-814e-015786ab29c7"},"cell_type":"markdown","source":"**Read the File**\n\nLet us first load the file. The file is formated as following:\n\nword1 [\"embedding vector\"]\n\nword2 [\"embedding vector\"]\n\n-- and so on\n\nLets read the dataset to show the magic of embedding"},{"metadata":{"collapsed":true,"_uuid":"d9bb7822c1027728c5591be8d479dc5cc22f9cdb","_cell_guid":"1d0369c9-b7fa-48c9-9dde-5f6fa01b662b","trusted":false},"cell_type":"code","source":"vocab, w2v = read_data(\"../input/glove.6B.50d.txt\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dff52554efaf228356a28208db4f02390299bc0","_cell_guid":"04687e81-d99e-4e0d-9da1-9977432d43ae"},"cell_type":"markdown","source":"# APPLICATION  : Similarity Score\n\nLets try to get similarty score between similar words like King and Queen or Baby and mother etc.. "},{"metadata":{"_uuid":"f68d0f6650cfc3ec064bbc2623cddbe98ef102c4","_cell_guid":"bcb114c3-b3bb-4559-828a-341249731469"},"cell_type":"markdown","source":"** Distance Metrics ** \n\nSo every word is converted to a vector and to check the closeness we can use any similarity score like L2 or cosine. Since cosine similarities are more used lets implement cosine similarity.\n\nWe Define cosine similarity as following:\n\n$$\\text{Cos Similarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$"},{"metadata":{"collapsed":true,"_uuid":"5f89566fb61a9db848f088da2f0c8af929fe6896","_cell_guid":"5fce0649-8183-469e-a2ab-b8c62d3ebf1e","trusted":false},"cell_type":"code","source":"def cos_sim(u,v):\n    \"\"\"\n    u: vector of 1st word\n    v: vector of 2nd Word\n    \"\"\"\n    numerator_ = u.dot(v)\n    denominator_= np.sqrt(np.sum(np.square(u))) * np.sqrt(np.sum(np.square(v)))\n    return numerator_/denominator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ff2b396ee2410449c3b01def26516d5e1f306d","_cell_guid":"21ad776f-d4bb-4314-8c63-51c957d0e9d6"},"cell_type":"markdown","source":"** Examples for Fun**"},{"metadata":{"collapsed":true,"_uuid":"cf13aca55ed5b358d9026bd67ab8665d0245fe86","_cell_guid":"aef907b5-b1c8-4158-9b18-396a0866e4d4","trusted":false},"cell_type":"code","source":"all_words = w2v.keys()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"da474ea1237195ecaa173ca7e8c4e8d5f4846747","_cell_guid":"42c3c23d-3ec0-47c3-ab3b-c720470f4322","trusted":false},"cell_type":"code","source":"print(\"Similarity Score of King and Queen\",cos_sim(w2v['king'],w2v['queen']))\nprint(\"Similarity Score of Mother and Pizza\",cos_sim(w2v['mother'],w2v['pizza']))\nprint(\"Similarity Score of Man and Pizza\",cos_sim(w2v['man'],w2v['pizza']))\nprint(\"Similarity Score of Mother and baby\",cos_sim(w2v['mother'],w2v['baby']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40cba7504ff103ba551983da68b9330d7eeace37","_cell_guid":"b1386f23-2e8d-47e8-b804-75bd341cc399"},"cell_type":"markdown","source":"# Visualization of Word Embedding"},{"metadata":{"collapsed":true,"_uuid":"bd40e4fc35a2aa7b3051cbf7d0ebcf4092184624","_cell_guid":"5b723e89-9c07-42c6-b61d-2d9273fc9644","trusted":false},"cell_type":"code","source":"def return_matrix(random_words,dim =50):\n    word_matrix = np.random.randn(len(random_words),dim)\n    i = 0\n    for word in random_words:\n        word_matrix[i] = w2v[word]\n        i +=1\n    return word_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0f126a0eb3405e46a0f1c465cfd5bc01458a09b","_cell_guid":"4c2ed0a9-4e85-41d1-9d4e-f533395d63f2"},"cell_type":"markdown","source":"## Visualization Using PCA"},{"metadata":{"collapsed":true,"_uuid":"d53b119934631aa5b71c60e844f5c737d7862056","_cell_guid":"80a82324-0ed8-4611-832b-530efd44400a","trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 15, 10\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b3749f74caac9c177b37ea1a10a292cd59c4d551","_cell_guid":"466d2ee5-53ef-42d5-b199-47996a240d57","trusted":false},"cell_type":"code","source":"random_words = ['man','woman','king','queen','microwave','baby','boy','girl','pizza','royal','mother','father','doctor','cook','throne']\nreturn_matrix_ = return_matrix(random_words)\npca_ = PCA(n_components=2)\nviz_data = pca_.fit_transform(return_matrix_) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a0d53683677e45837deb154533fda5930206f6ef","_cell_guid":"ce86fd70-86a3-416e-81dc-a49225f18900","trusted":false},"cell_type":"code","source":"plt.scatter(viz_data[:,0],viz_data[:,1],cmap=plt.get_cmap('Spectral'))\nfor label,x,y in zip(random_words,viz_data[:,0],viz_data[:,1]):\n    plt.annotate(\n        label,\n        xy=(x,y),\n        xytext=(-14, 14),\n        textcoords='offset points',\n        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0')\n    )\nplt.xlabel('PCA Component 1 ')\nplt.ylabel('PCA Component 2')\nplt.title('PCA representation for Word Embedding')\nplt.xlim(-10,10)\nplt.ylim(-5,6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04f0f975039a41fb8ec9016722bd18da8c4d1da3","_cell_guid":"0265027d-0bf2-4da4-bffe-932ad807a50c"},"cell_type":"markdown","source":"## Visualization using T-SNE "},{"metadata":{"collapsed":true,"_uuid":"9e1e16f41663088738adecfa8f9574bd3a0d5f22","_cell_guid":"25d635ce-83b1-4014-939f-636e62692d78","trusted":false},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1,perplexity=3,method='exact')\ntsne_results = tsne.fit_transform(return_matrix_)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"ae1f0e389349e52252e37377fda555379e8298c9","_cell_guid":"584859f0-8161-40b9-850d-12968cd409fe","trusted":false},"cell_type":"code","source":"plt.scatter(tsne_results[:,0],tsne_results[:,1],cmap=plt.get_cmap('Spectral'))\nfor label,x,y in zip(random_words,tsne_results[:,0],tsne_results[:,1]):\n    plt.annotate(\n        label,\n        xy=(x,y),\n        xytext=(-14, 14),\n        textcoords='offset points',\n        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0')\n    )\nplt.xlabel('TSNE Component 1 ')\nplt.ylabel('TSNE Component 2')\nplt.title('TSNE representation for Word Embedding')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ab36f94ad819f6e02498f87a186dc2edfddc214","_cell_guid":"c9408dd4-cd61-420f-86ef-348543b9e4b3"},"cell_type":"markdown","source":"We can see how related words are grouped together. Amazing right!!"},{"metadata":{"_uuid":"bac3e25c1c7b9c286abebf15af3228e39303fa97","_cell_guid":"688f62a8-11a0-4b4b-b14c-cbfdd43239aa"},"cell_type":"markdown","source":"# Application : Analogy \n\nLets try another application where we try analogy i.e. \"Man is to woman as King is to ?\" or \"India is to Delhi as Japan is to ?\". \n\nIntution is the new word should be close to (word3 - (word1 - word2)) where word3 = doctor, word1 = cook, word1 = pizza in \"cook is to pizza as doctor is to ?\"."},{"metadata":{"collapsed":true,"_uuid":"e5dfad5089d365ddbcff62e507cfff6bc74236ea","_cell_guid":"625b41cc-bbdc-47b2-8052-e095fa6937ef","trusted":false},"cell_type":"code","source":"def find_w4(word1,word2,word3, w2v):\n    \"\"\"\n    \"\"\"\n    word_list = w2v.keys()\n    max_sim = -1000\n    #Make Sure they are lower\n    word1,word2,word3 = word1.lower(),word2.lower(),word3.lower()\n    diff_vec = w2v[word3] - (w2v[word1]-w2v[word2]) #word3 - (word1 - word2)\n    for word in word_list:\n        vec = w2v[word]\n        sim_ = cos_sim(u=diff_vec,v=vec)\n        if sim_ > max_sim:\n            max_sim = sim_\n            word_selected =  word\n            \n    return word_selected","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"75e020ba18c822bcb1222a48722c7a53e59e475f","_cell_guid":"cbe2fec8-e39d-4894-a23d-4e9af7c5b4d0","trusted":false},"cell_type":"code","source":"print(\"King is to Queen as Man is to \",find_w4('king','queen','man',w2v))\nprint(\"Cook is to Pizza as Doctor is to \",find_w4('cook','pizza','doctor',w2v))\nprint(\"India is to Delhi as Japan is to \",find_w4('india','delhi','japan',w2v))\nprint(\"kid is to toy as doctor is to \",find_w4('kid','toy','doctor',w2v))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41f3610da20f7c88660938eabe95aad19baab6f6","_cell_guid":"66e3d570-a1ad-4f23-82e5-840c7c1cebd7"},"cell_type":"markdown","source":"**Cook is to Pizza as Doctor is to  pizza **\n\nDoctor is to Pizza!! Haha.. This was a secret I guess. Well machine learning can be funny. \n\n![](https://9bf6ddc20002c5f1a946-ef07da46c7e506e973e0d9fa57c693df.ssl.cf1.rackcdn.com/636445631371205663+32594.png)"},{"metadata":{"collapsed":true,"_uuid":"bc23abd3e36a892197eb3e5a38f5b62cc8bfd7af","_cell_guid":"773f91f3-c754-4856-b836-abacb3fb4c70","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}