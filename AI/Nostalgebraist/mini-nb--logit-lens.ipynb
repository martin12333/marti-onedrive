{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01b0785-7c27-436a-ae6d-fd63980f2a19",
   "metadata": {},
   "source": [
    "### the logit lens on gpt2 activations\n",
    "https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc1e06b-88e3-4b5a-a2ab-31c7c16489d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini nb  #mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34da626-59a8-4809-b3af-0c919c78aee1",
   "metadata": {
    "id": "ZCUtalZSD9T1"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee9490c-8dad-4c80-bc82-65c9119ce3d9",
   "metadata": {
    "id": "2yAXt5hVERl6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt3_abstract = \"\"\"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\n",
    "on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\n",
    "in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\n",
    "thousands of examples. By contrast, humans can generally perform a new language task from only\n",
    "a few examples or from simple instructions – something which current NLP systems still largely\n",
    "struggle to do. Here we show that scaling up language models greatly improves task-agnostic,\n",
    "few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion\n",
    "parameters, 10x more than any previous non-sparse language model, and test its performance in\n",
    "the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\n",
    "with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\n",
    "achieves strong performance on many NLP datasets, including translation, question-answering, and\n",
    "cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\n",
    "unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\n",
    "time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\n",
    "datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\n",
    "we find that GPT-3 can generate samples of news articles which human evaluators have difficulty\n",
    "distinguishing from articles written by humans. We discuss broader societal impacts of this finding\n",
    "and of GPT-3 in general.\"\"\".replace(\"\\n\", \" \")\n",
    "\n",
    "plasma = \"\"\"Sometimes, when people say plasma, they mean a state of matter. Other times, when people say plasma\"\"\"\n",
    "plasma_short = \"\"\"That's my first example of plasma. My second example of\"\"\"\n",
    "\n",
    "plasma_repetitive = \"\"\"I love plasma. I love plasma. I love plasma. I love plasma.\"\"\"\n",
    "\n",
    "seals = \"\"\"What the fuck did you just fucking say about me, you little bitch? I'll have you know I graduated top of my class in the Navy Seals, and I've been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. I am trained in gorilla warfare and I'm the top sniper in the entire US armed forces. You are nothing to me but just another target. I will wipe you the fuck out with\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bca2b13-563e-4f9d-800d-803f08688a35",
   "metadata": {
    "id": "MyhMrB8aEaAV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def text_to_input_ids(text):\n",
    "    toks = tokenizer.encode(text)\n",
    "    return torch.as_tensor(toks).view(1, -1)           #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03cab273-a401-428a-ae32-262b34fc540c",
   "metadata": {
    "id": "HbKmvieBE00n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 160])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = text_to_input_ids(gpt3_abstract)\n",
    "\n",
    "input_ids = input_ids[:, :160]\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae35749-cdbb-4612-bb6c-ce0f66492ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9a390-7411-4d11-b661-e71a839aa315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b1cbe-6c0b-46ba-b3af-b0f73262fed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
